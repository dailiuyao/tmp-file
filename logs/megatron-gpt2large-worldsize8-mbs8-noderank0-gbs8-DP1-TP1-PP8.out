using world size: 8, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
using torch.float16 for parameters ...
Gradient accumulation fusion to linear layer weight gradient computation is supported only with fp32 gradient accumulation. Setting gradient_accumulation_fusion to False
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/home/yuke/lyd/Megatron-LM/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_seq_length .............................. 512
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 4800
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 8
  gradient_accumulation_fusion .................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1200
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 50
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /local/scratch/checkpoints/gpt2_774m
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 64
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 512
  merge_file ...................................... /home/yuke/lyd/Megatron-LM/model/gpt2-merges.txt
  micro_batch_size ................................ 8
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 24
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 40
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 8
  pipeline_model_parallel_split_rank .............. None
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /local/scratch/checkpoints/gpt2_774m
  save_interval ................................... 50
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 512
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 100
  train_samples ................................... None
  transformer_pipeline_model_parallel_size ........ 8
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /home/yuke/lyd/Megatron-LM/model/gpt2-vocab.json
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 8
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory '/home/yuke/lyd/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/yuke/lyd/Megatron-LM/megatron/data'
>>> done with dataset index builder. Compilation time: 0.945 seconds
> compiling and loading fused kernels ...
[1/3] c++ -MMD -MF scaled_upper_triang_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o scaled_upper_triang_masked_softmax.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[2/3] /soft/compilers/cudatoolkit/cuda-11.8.0/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -o scaled_upper_triang_masked_softmax_cuda.cuda.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.8.0/lib64 -lcudart -o scaled_upper_triang_masked_softmax_cuda.so
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[1/3] c++ -MMD -MF scaled_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax.cpp -o scaled_masked_softmax.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[2/3] /soft/compilers/cudatoolkit/cuda-11.8.0/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -o scaled_masked_softmax_cuda.cuda.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.8.0/lib64 -lcudart -o scaled_masked_softmax_cuda.so
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[1/3] c++ -MMD -MF scaled_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_softmax.cpp -o scaled_softmax.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[2/3] /soft/compilers/cudatoolkit/cuda-11.8.0/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_softmax_cuda.cu -o scaled_softmax_cuda.cuda.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[3/3] c++ scaled_softmax.o scaled_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.8.0/lib64 -lcudart -o scaled_softmax_cuda.so
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[1/3] c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda.cpp -o layer_norm_cuda.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
[2/3] /soft/compilers/cudatoolkit/cuda-11.8.0/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.8.0/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -maxrregcount=50 -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o 
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function 'void cuda_layer_norm(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double)':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:224: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:272: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:296: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                        ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:359: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:414: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:119: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:142: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                              ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:167: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:191: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                               ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:258: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                  ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:317: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                             ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:131: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                   ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:154: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                          ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:179: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                   ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:203: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                           ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:274: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                  ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:337: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                 ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:151: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                              ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:199: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:227: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                   ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:294: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:353: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                 ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:166: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:189: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                             ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:214: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:246: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:317: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                             ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:380: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                            ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function 'void cuda_layer_norm_gradient(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double, at::Tensor*, at::Tensor*, at::Tensor*)':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:224: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:272: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:333: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                             ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:389: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:438: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:489: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:550: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:120: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                        ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:143: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                               ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:168: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                        ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:233: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                         ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:293: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:342: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                      ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:397: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                             ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:462: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:132: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                    ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:155: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                           ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:180: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                    ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:249: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                         ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:313: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                         ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:362: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                          ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:421: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:490: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:152: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                        ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:175: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                               ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:200: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                        ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:265: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                         ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:325: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:378: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                          ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:433: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:498: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:167: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:190: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                              ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:215: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                       ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:284: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                            ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:348: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                            ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:405: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:464: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:533: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = float]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:562:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::Half]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:474:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::BFloat16]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:502:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::Half; U = float; V = c10::Half]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:510:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::BFloat16; U = float; V = c10::BFloat16]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:545:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
[3/3] c++ layer_norm_cuda.o layer_norm_cuda_kernel.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.8.0/lib64 -lcudart -o fused_mix_prec_layer_norm_cuda.so
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
/bin/sh: /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)
x3110c0s7b0n0:1174:1174 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1174:1174 [0] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3110c0s7b0n0:1174:1174 [0] NCCL INFO Bootstrap : Using hsn0:10.201.2.254<0>
x3110c0s7b0n0:1174:1174 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3110c0s7b0n0:1174:1174 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.14.3+cuda11.8
x3110c0s7b0n0:1174:1174 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14d035e00000
x3110c0s7b0n0:1177:1177 [3] NCCL INFO cudaDriverVersion 11080
x3110c0s7b0n0:1176:1176 [2] NCCL INFO cudaDriverVersion 11080
x3110c0s7b0n0:1175:1175 [1] NCCL INFO cudaDriverVersion 11080
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NET/IB : No device found.
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.201.2.254<0>
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:1176 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1176:1176 [2] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3110c0s7b0n0:1176:1176 [2] NCCL INFO Bootstrap : Using hsn0:10.201.2.254<0>
x3110c0s7b0n0:1177:1177 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1177:1177 [3] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3110c0s7b0n0:1177:1177 [3] NCCL INFO Bootstrap : Using hsn0:10.201.2.254<0>
x3110c0s7b0n0:1177:1177 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3110c0s7b0n0:1177:1177 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x149959800000
x3110c0s7b0n0:1176:1176 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3110c0s7b0n0:1176:1176 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x148b75e00000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NET/IB : No device found.
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.201.2.254<0>
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NET/IB : No device found.
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.201.2.254<0>
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Using network Socket
x3110c0s7b0n0:1175:1175 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1175:1175 [1] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3110c0s7b0n0:1175:1175 [1] NCCL INFO Bootstrap : Using hsn0:10.201.2.254<0>
x3110c0s7b0n0:1175:1175 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3110c0s7b0n0:1175:1175 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14f5e1800000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NET/IB : No device found.
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.201.2.254<0>
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1176:44604 [2] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x148b76800000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1176:44604 [2] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO ==========================================
x3110c0s7b0n0:1176:44604 [2] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:44604 [2] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1176:44604 [2] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:44604 [2] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:44604 [2] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1175:44608 [1] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x14f5de200000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1177:44603 [3] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x149956200000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1177:44603 [3] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:44603 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO ==========================================
x3110c0s7b0n0:1177:44603 [3] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:44603 [3] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1177:44603 [3] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:44603 [3] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:44603 [3] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1177:44603 [3] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1177:44603 [3] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1175:44608 [1] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO ==========================================
x3110c0s7b0n0:1175:44608 [1] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:44608 [1] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1175:44608 [1] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:44608 [1] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:44608 [1] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1174:44576 [0] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x14d036800000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1174:44576 [0] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO ==========================================
x3110c0s7b0n0:1174:44576 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:44576 [0] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1174:44576 [0] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:44576 [0] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:44576 [0] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/4/-1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Tree 1 : 4 -> 0 -> 1/-1/-1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 00/02 :    0   3   2   5   4   7   6   1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 01/02 :    0   3   2   5   4   7   6   1
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Ring 00 : 1 -> 0 -> 3
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Ring 01 : 1 -> 0 -> 3
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/-1/-1->0->4
x3110c0s7b0n0:1174:44576 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1174:44576 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14d036800000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14d036800e00
x3110c0s7b0n0:1174:44576 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14d036801000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14d036801e00
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14cfe4000c60
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Ring 00 : 6 -> 1 -> 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Ring 01 : 6 -> 1 -> 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1175:44608 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14f5de200000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14f5de200e00
x3110c0s7b0n0:1175:44608 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14f5de201000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14f5de201e00
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000c80
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d036a00000
x3110c0s7b0n0:1175:44681 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14f5d0000c60
x3110c0s7b0n0:1175:44681 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-6IwT32

x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 0 from local rank 1, transport 2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000c80
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Ring 00 : 3 -> 2 -> 5
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Ring 01 : 3 -> 2 -> 5
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
x3110c0s7b0n0:1176:44604 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1176:44604 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x148b76800000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x148b76800e00
x3110c0s7b0n0:1176:44604 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x148b76801000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x148b76801e00
x3110c0s7b0n0:1176:44672 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x148b1c000c60
x3110c0s7b0n0:1176:44672 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-h1mJs9

x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 0 from local rank 2, transport 2
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000c80
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 00/0 : 2[85000] -> 5[46000] [send] via NET/Socket/0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 1 from local rank 2, transport 2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 1 from local rank 0, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Ring 00 : 0 -> 3 -> 2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Ring 01 : 0 -> 3 -> 2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
x3110c0s7b0n0:1177:44603 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1177:44603 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x149956200000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x149956200e00
x3110c0s7b0n0:1177:44603 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x149956201000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x149956201e00
x3110c0s7b0n0:1177:44676 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x149948000c60
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 0 from local rank 3, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000c80
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149956400000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 00/0 : 6[85000] -> 1[46000] [receive] via NET/Socket/0
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 1 from local rank 1, transport 2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000cc0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000cc0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d037000000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000cc0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 01/0 : 2[85000] -> 5[46000] [send] via NET/Socket/0
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 1 from local rank 3, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 01/0 : 6[85000] -> 1[46000] [receive] via NET/Socket/0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000cc0
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149956a00000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy send connection 2 from local rank 3, transport 0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy recv connection 2 from local rank 0, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000d00
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14f5de400000
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14994e000000
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x148b76a00000
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x148b77400000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000d00
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14d038200000
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14f5dee00000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy send connection 2 from local rank 1, transport 0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 2 from local rank 2, transport 0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy recv connection 3 from local rank 0, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000d00
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f5df800000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy send connection 3 from local rank 3, transport 0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000d00
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148b77e00000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000d40
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14d038800000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy send connection 3 from local rank 1, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000d40
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14994e600000
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 3 from local rank 2, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000d40
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f5ce000000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000d40
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148b78400000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connected all rings
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 4 from local rank 1, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000d80
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 0
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f5cf200000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000d80
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d039a00000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 4 from local rank 3, transport 0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 4 from local rank 2, transport 0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 5 from local rank 0, transport 0
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 5 from local rank 1, transport 0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000dc0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d03a000000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000d80
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14994f800000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000d80
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148b79600000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000dc0
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f5cf800000
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 5 from local rank 3, transport 0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 5 from local rank 2, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000dc0
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149946000000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy send connection 6 from local rank 1, transport 0
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000dc0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148b79c00000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 6 from local rank 2, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000e00
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f5cc000000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000e00
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b7a200000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy send connection 7 from local rank 1, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000e40
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f5cc600000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 7 from local rank 2, transport 0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-MM5NRX

x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy recv connection 6 from local rank 0, transport 2
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000e40
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b7a800000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000e00
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 8 from local rank 2, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1177:44603 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1177:44603 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1177:44676 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-31RXqd

x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy send connection 6 from local rank 3, transport 2
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 8 from local rank 1, transport 0
x3110c0s7b0n0:1177:44603 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000e00
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x149947200000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000e80
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b7c600000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy recv connection 7 from local rank 0, transport 2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000e80
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f5ca600000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 9 from local rank 2, transport 0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000e40
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000ec0
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b7cc00000
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 9 from local rank 1, transport 0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000ec0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 01/0 : 4[7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 8 from local rank 0, transport 2
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f5cac00000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000e80
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/Socket/0
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 9 from local rank 0, transport 2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000ec0
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Channel 01/0 : 0[7000] -> 4[7000] [send] via NET/Socket/0
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1175:44608 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1175:44608 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1176:44604 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1176:44604 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy send connection 10 from local rank 2, transport 2
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy send connection 10 from local rank 1, transport 2
x3110c0s7b0n0:1175:44608 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000f00
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14f5c8000000
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14d03b200000
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14d03bc00000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000f00
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x148b7de00000
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14d03c600000
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14d03d000000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            512 |              0 |              0 |            512 |              0 |              0 |            512 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO     AllReduce |    20.0/   3.1 |    28.9/   0.0 |   224.0/  11.8 |    22.9/   3.4 |    45.0/   0.0 |    80.8/  13.7 |     7.4/   0.0 |     7.4/   0.0 |    29.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |    48.0/   0.0 |
x3110c0s7b0n0:1174:44576 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1174:44576 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 10 from local rank 0, transport 2
x3110c0s7b0n0:1174:44576 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000f00
x3110c0s7b0n0:1175:44608 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14f5de202000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x149956202000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14993e000000
x3110c0s7b0n0:1177:44603 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x149959800200
x3110c0s7b0n0:1177:44603 [3] NCCL INFO comm 0x56257d333060 rank 3 nranks 8 cudaDev 3 busId c7000 - Init COMPLETE
x3110c0s7b0n0:1175:44608 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14f5c6000000
x3110c0s7b0n0:1175:44608 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14f5e1800200
x3110c0s7b0n0:1175:44608 [1] NCCL INFO comm 0x55d66c5036d0 rank 1 nranks 8 cudaDev 1 busId 46000 - Init COMPLETE
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14d03da00000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14d036802000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x148b76802000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14d03de00000
x3110c0s7b0n0:1174:44576 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14d035e00200
x3110c0s7b0n0:1174:44576 [0] NCCL INFO comm 0x564802052d90 rank 0 nranks 8 cudaDev 0 busId 7000 - Init COMPLETE
x3110c0s7b0n0:1176:44604 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x148b7e200000
x3110c0s7b0n0:1176:44604 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x148b75e00200
x3110c0s7b0n0:1176:44604 [2] NCCL INFO comm 0x555779502ba0 rank 2 nranks 8 cudaDev 2 busId 85000 - Init COMPLETE
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x1499a7200200 recvbuff 0x1499a7200200 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
x3110c0s7b0n0:1177:1177 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14f62b200200 recvbuff 0x14f62b200200 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
x3110c0s7b0n0:1175:1175 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d043200200 recvbuff 0x14d043200200 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200
x3110c0s7b0n0:1174:1174 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000068000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000021000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200200
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200200
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x148b83200200 recvbuff 0x148b83200200 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
x3110c0s7b0n0:1176:1176 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 121850 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 68385 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 109631 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 79272 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 1 sendbuff 0x14d043200000 recvbuff 0x14d043200000 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000039000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000019000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 1 sendbuff 0x1499a7200000 recvbuff 0x1499a7200000 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 1 sendbuff 0x148b83200000 recvbuff 0x148b83200000 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 1 sendbuff 0x14f62b200000 recvbuff 0x14f62b200000 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 792057 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 801454 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 795359 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 2761064 us
>>> done with compiling and loading fused kernels. Compilation time: 431.228 seconds
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 2 sendbuff 0x14f62b200000 recvbuff 0x14f62b200000 count 1 datatype 8 op 3 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 2 sendbuff 0x148b83200000 recvbuff 0x148b83200000 count 1 datatype 8 op 3 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclFloat64/ncclDouble, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 2 sendbuff 0x14d043200000 recvbuff 0x14d043200000 count 1 datatype 8 op 3 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000034000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000018000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 2 sendbuff 0x1499a7200000 recvbuff 0x1499a7200000 count 1 datatype 8 op 3 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_FUNC_NAME (common.h)] 3 AllReduce TREE LL Min double Elapsed time: 4 us

[LYD INFO in NCCL_FUNC_NAME (common.h)] 2 AllReduce TREE LL Min double Elapsed time: 383 us

[LYD INFO in NCCL_FUNC_NAME (common.h)] 1 AllReduce TREE LL Min double Elapsed time: 509 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum double Elapsed time: 71351 us
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 3 sendbuff 0x1499a7200200 recvbuff 0x1499a7200200 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum double Elapsed time: 71992 us
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 3 sendbuff 0x14f62b200200 recvbuff 0x14f62b200200 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_FUNC_NAME (common.h)] 0 AllReduce TREE LL Min double Elapsed time: 71610 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum double Elapsed time: 71702 us
time to initialize megatron (seconds): 438.756
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 3 sendbuff 0x14d043200200 recvbuff 0x14d043200200 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000028000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000017000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200200
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200200
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum double Elapsed time: 71483 us
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 3 sendbuff 0x148b83200200 recvbuff 0x148b83200200 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 49896 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 24482 us
[after megatron is initialized] datetime: 2023-12-19 01:30:16 
building GPT model ...

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 38394 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 29064 us
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 86478000
x3110c0s7b0n0:1174:1174 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14d035e00400
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 86478000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 86478000
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Using network Socket
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 4 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 4 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 4 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1174:45808 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1174:45808 [0] NCCL INFO === System : maxBw 24.0 totalBw 24.0 ===
x3110c0s7b0n0:1174:45808 [0] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:45808 [0] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1174:45808 [0] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO ==========================================
x3110c0s7b0n0:1174:45808 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:45808 [0] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type LOC/SYS, sameChannels 1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO  0 : NET/0 GPU/0 NET/0
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 1, bw 48.000000/24.000000, type LOC/SYS, sameChannels 1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO  0 : NET/0 GPU/0 NET/0
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Tree 1 : 1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 00/02 :    0   1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 01/02 :    0   1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Ring 00 : 1 -> 0 -> 1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Ring 01 : 1 -> 0 -> 1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
x3110c0s7b0n0:1174:45808 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1174:45808 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x14d036803600
x3110c0s7b0n0:1174:45808 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x14d036803c00
x3110c0s7b0n0:1174:45808 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x14d036803e00
x3110c0s7b0n0:1174:45808 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x14d036804400
x3110c0s7b0n0:1174:45822 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14d000000d10
x3110c0s7b0n0:1174:45822 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-RsipOW

x3110c0s7b0n0:1174:45822 [0] NCCL INFO New proxy recv connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14d000000d30
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 00/0 : 1[c7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:45822 [0] NCCL INFO New proxy recv connection 1 from local rank 0, transport 2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14d000000d70
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 01/0 : 1[c7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:45822 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14d000000db0
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[c7000] [send] via NET/Socket/0
x3110c0s7b0n0:1174:45822 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14d000000df0
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[c7000] [send] via NET/Socket/0
x3110c0s7b0n0:1174:45822 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14cffe000000
x3110c0s7b0n0:1174:45822 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14cffea00000
x3110c0s7b0n0:1174:45822 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14cfff400000
x3110c0s7b0n0:1174:45822 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14cffc000000
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            512 |              0 |              0 |            512 |              0 |              0 |            512 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/  12.0 |    14.0/   0.0 |    18.0/  48.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/  12.0 |    14.0/   0.0 |    18.0/  48.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO     AllReduce |    14.4/   3.1 |    21.4/   0.0 |    56.0/  11.8 |    10.8/   6.0 |    21.0/   0.0 |    35.4/  24.0 |     4.4/   0.0 |     4.4/   0.0 |    10.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45808 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
x3110c0s7b0n0:1174:45808 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1174:45822 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 2
x3110c0s7b0n0:1174:45808 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14d000000e30
x3110c0s7b0n0:1174:45808 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14d036804600
x3110c0s7b0n0:1174:45808 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14cfb6000000
x3110c0s7b0n0:1174:45808 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14d035e00600
x3110c0s7b0n0:1174:45808 [0] NCCL INFO comm 0x5648027f61b0 rank 0 nranks 2 cudaDev 0 busId 7000 - Init COMPLETE
x3110c0s7b0n0:1174:45822 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14cffca00000
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 60364800, datatype: ncclFloat16/ncclHalf, message size: 120729600 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d00c000000 recvbuff 0x14d00c000000 count 60364800 datatype 6 op 0 root 0 comm 0x5648027f61b0 [nranks=2] stream 0x5648013dc300
x3110c0s7b0n0:1174:1174 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000057000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000019000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d00c000000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d00c000000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 60364800
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 6
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 544
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 120729600
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 147457200

[LYD INFO in NCCL_FUNC_NAME (common.h)] 0 AllReduce RING SIMPLE Sum half Elapsed time: 851507 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce RING LL Sum half Elapsed time: 851606 us
> learning rate decay style: cosine
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 4 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000018000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 14 us
WARNING: could not find the metadata file /local/scratch/checkpoints/gpt2_774m/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 5 sendbuff 0x14d043228800 recvbuff 0x14d043228800 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000027000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000017000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043228800
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043228800
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 2082239 us
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 5 sendbuff 0x1499a7228800 recvbuff 0x1499a7228800 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 2053710 us
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 5 sendbuff 0x148b83228800 recvbuff 0x148b83228800 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 2063763 us
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 5 sendbuff 0x14f62b228800 recvbuff 0x14f62b228800 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 8 us
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 6 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 9980 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 40320 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 42742 us
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 6 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 6 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000038000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000018000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 6 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 48933 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 23562 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 36586 us
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-12-19 01:30:18 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      800
    validation: 88
    test:       8
> building train, validation, and test datasets for GPT ...
 > building dataset index ...

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 57121 us
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.016343 seconds
    number of documents: 5263
 > dataset split:
    train:
     document indices in [0, 5100) total of 5100 documents
    validation:
     document indices in [5100, 5258) total of 158 documents
    test:
     document indices in [5258, 5263) total of 5 documents
NCCL version 2.14.3+cuda11.8
x3110c0s7b0n0:1176:1176 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x148b75e00400
NCCL version 2.14.3+cuda11.8
x3110c0s7b0n0:1175:1175 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14f5e1800400
NCCL version 2.14.3+cuda11.8
x3110c0s7b0n0:1177:1177 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x149959800400
x3110c0s7b0n0:1174:1174 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14d035e00800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Using network Socket
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Using network Socket
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Using network Socket
x3110c0s7b0n0:1177:45912 [3] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1177:45912 [3] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1177:45912 [3] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1177:45912 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:45912 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:45912 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (0)
x3110c0s7b0n0:1177:45912 [3] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1177:45912 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO ==========================================
x3110c0s7b0n0:1177:45912 [3] NCCL INFO GPU/C7000 :GPU/C7000 (0/5000.000000/LOC) CPU/0 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1177:45912 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956203600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956203a00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956203c00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956204000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956204200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956204600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956204800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956204c00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956204e00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956205200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956205400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956205800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956205a00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956205e00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956206000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956206400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956206600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956206a00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956206c00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956207000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956207200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956207600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956207800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956207c00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956207e00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956208200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956208400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956208800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956208a00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956208e00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956209000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956209400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956209600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956209a00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956209c00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620a000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620a200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620a600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620a800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620ac00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620ae00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620b200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620b400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620b800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620ba00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620be00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620c000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620c400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620c600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620ca00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620cc00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620d000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620d200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620d600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620d800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620dc00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620de00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620e200
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620e400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620e800
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620ea00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620ee00
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995620f000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995620f400
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1177:45912 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1177:45924 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x1498fc000cf0
x3110c0s7b0n0:1177:45924 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-BMKRNd

x3110c0s7b0n0:1177:45924 [3] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1177:45912 [3] NCCL INFO Connection to proxy localRank 0 -> connection 0x1498fc000d10
x3110c0s7b0n0:1176:45908 [2] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1176:45908 [2] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1176:45908 [2] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1176:45908 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:45908 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1176:45908 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (0)
x3110c0s7b0n0:1176:45908 [2] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1176:45908 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO ==========================================
x3110c0s7b0n0:1176:45908 [2] NCCL INFO GPU/85000 :GPU/85000 (0/5000.000000/LOC) CPU/1 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1176:45908 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76803600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76803a00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76803c00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76804000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76804200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76804600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76804800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76804c00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76804e00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76805200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76805400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76805800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76805a00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76805e00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76806000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76806400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76806600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76806a00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76806c00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76807000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76807200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76807600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76807800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76807c00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76807e00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76808200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76808400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76808800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76808a00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76808e00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76809000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76809400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76809600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76809a00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76809c00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680a000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680a200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680a600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680a800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680ac00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680ae00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680b200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680b400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680b800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680ba00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680be00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680c000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680c400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680c600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680ca00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680cc00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680d000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680d200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680d600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680d800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680dc00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680de00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680e200
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680e400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680e800
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680ea00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680ee00
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7680f000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7680f400
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Connected all rings
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1176:45908 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1177:45912 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14995620f600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1174:45915 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1174:45915 [0] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1174:45915 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:45915 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45915 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1174:45915 [0] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1174:45915 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:45915 [0] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO ==========================================
x3110c0s7b0n0:1174:45915 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1174:45915 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036805c00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036806000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036806200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036806600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036806800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036806c00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036806e00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036807200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036807400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036807800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036807a00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036807e00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036808000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036808400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036808600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036808a00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036808c00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036809000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036809200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036809600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036809800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036809c00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036809e00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680a200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680a400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680a800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680aa00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680ae00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680b000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680b400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680b600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680ba00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680bc00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680c000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680c200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680c600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680c800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680cc00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680ce00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680d200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680d400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680d800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680da00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680de00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680e000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680e400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680e600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680ea00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680ec00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680f000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680f200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680f600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680f800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03680fc00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03680fe00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036810200
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036810400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036810800
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036810a00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036810e00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036811000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036811400
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036811600
x3110c0s7b0n0:1174:45915 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036811a00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1174:45915 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1175:45910 [1] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1175:45910 [1] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1175:45910 [1] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1175:45910 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1175:45910 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (0)
x3110c0s7b0n0:1175:45910 [1] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO ==========================================
x3110c0s7b0n0:1175:45910 [1] NCCL INFO GPU/46000 :GPU/46000 (0/5000.000000/LOC) CPU/2 (2/24.000000/PHB) 
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1175:45910 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de203600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de203a00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de203c00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de204000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de204200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de204600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de204800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de204c00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de204e00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de205200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de205400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de205800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de205a00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de205e00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de206000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de206400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de206600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de206a00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de206c00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de207000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de207200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de207600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de207800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de207c00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de207e00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de208200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de208400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de208800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de208a00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de208e00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de209000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de209400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de209600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de209a00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de209c00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20a000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20a200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20a600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20a800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20ac00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20ae00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20b200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20b400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20b800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20ba00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20be00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20c000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20c400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20c600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20ca00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20cc00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20d000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20d200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20d600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20d800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20dc00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20de00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20e200
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20e400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20e800
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20ea00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20ee00
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de20f000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de20f400
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Connected all rings
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1175:45910 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1176:45930 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x148ae0000cf0
x3110c0s7b0n0:1176:45930 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-dHGbr8

x3110c0s7b0n0:1176:45930 [2] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1177:45912 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x1498f2000000
x3110c0s7b0n0:1177:45912 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x149959800600
x3110c0s7b0n0:1177:45912 [3] NCCL INFO comm 0x56257e541070 rank 0 nranks 1 cudaDev 3 busId c7000 - Init COMPLETE
x3110c0s7b0n0:1177:45924 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x1498ee000000
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e541070 [nranks=1] stream 0x56257c69bfe0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000035000 seconds
x3110c0s7b0n0:1174:45935 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14cf68000cf0
x3110c0s7b0n0:1174:45935 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-kRmeGZ

x3110c0s7b0n0:1174:45935 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1175:45936 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14f580000cf0
x3110c0s7b0n0:1175:45936 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-GXAhl1

x3110c0s7b0n0:1175:45936 [1] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1175:45910 [1] NCCL INFO Connection to proxy localRank 0 -> connection 0x14f580000d10
x3110c0s7b0n0:1175:45910 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14f5de20f600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO Connection to proxy localRank 0 -> connection 0x148ae0000d10
x3110c0s7b0n0:1176:45908 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x148b7680f600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x148ade000000
x3110c0s7b0n0:1176:45908 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x148b75e00600
x3110c0s7b0n0:1176:45908 [2] NCCL INFO comm 0x55577a4b5d50 rank 0 nranks 1 cudaDev 2 busId 85000 - Init COMPLETE
x3110c0s7b0n0:1176:45930 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x148ad4000000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf68000d10
x3110c0s7b0n0:1174:45915 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14d036811c00
x3110c0s7b0n0:1175:45936 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14f578000000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14f586000000
x3110c0s7b0n0:1175:45910 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14f5e1800600
x3110c0s7b0n0:1175:45910 [1] NCCL INFO comm 0x55d66cc87de0 rank 0 nranks 1 cudaDev 1 busId 46000 - Init COMPLETE
x3110c0s7b0n0:1174:45915 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14cf66000000
x3110c0s7b0n0:1174:45915 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14d035e00a00
x3110c0s7b0n0:1174:45915 [0] NCCL INFO comm 0x5648029e40d0 rank 0 nranks 1 cudaDev 0 busId 7000 - Init COMPLETE
x3110c0s7b0n0:1174:45935 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14cf62000000
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x5648029e40d0 [nranks=1] stream 0x5648013dc440

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000040000 seconds
x3110c0s7b0n0:1177:1177 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x149959800800
x3110c0s7b0n0:1174:1174 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14d035e00c00
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4b5d50 [nranks=1] stream 0x5557788add70

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000035000 seconds
x3110c0s7b0n0:1176:1176 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x148b75e00800
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Using network Socket
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Using network Socket
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66cc87de0 [nranks=1] stream 0x55d66b8ae700

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000039000 seconds
x3110c0s7b0n0:1175:1175 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14f5e1800800
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Using network Socket
x3110c0s7b0n0:1177:45953 [3] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1177:45953 [3] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1177:45953 [3] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1177:45953 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO ==========================================
x3110c0s7b0n0:1177:45953 [3] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:45953 [3] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1177:45953 [3] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:45953 [3] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1177:45953 [3] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1177:45953 [3] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1177:45953 [3] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1174:45954 [0] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1174:45954 [0] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO ==========================================
x3110c0s7b0n0:1174:45954 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:45954 [0] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1174:45954 [0] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:45954 [0] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1174:45954 [0] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1175:45961 [1] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1175:45961 [1] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO ==========================================
x3110c0s7b0n0:1175:45961 [1] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:45961 [1] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1175:45961 [1] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:45961 [1] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1175:45961 [1] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1176:45958 [2] NCCL INFO === System : maxBw 24.0 totalBw 264.0 ===
x3110c0s7b0n0:1176:45958 [2] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO               + NET[25.0] - NET/0 (0/0/25.000000)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO ==========================================
x3110c0s7b0n0:1176:45958 [2] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:45958 [2] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (4/24.000000/PHB) 
x3110c0s7b0n0:1176:45958 [2] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:45958 [2] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/0 (5/24.000000/SYS) 
x3110c0s7b0n0:1176:45958 [2] NCCL INFO NET/0 :GPU/7000 (5/24.000000/SYS) GPU/46000 (4/24.000000/PHB) GPU/85000 (5/24.000000/SYS) GPU/C7000 (5/24.000000/SYS) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 24.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO  0 : NET/0 GPU/1 GPU/0 GPU/3 GPU/2 NET/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 48.000000/24.000000, type NVL/SYS, sameChannels 1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO  0 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Ring 00 : 3 -> 2 -> 5
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Ring 01 : 3 -> 2 -> 5
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
x3110c0s7b0n0:1176:45958 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1176:45958 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x148b76810c00
x3110c0s7b0n0:1176:45958 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x148b76811a00
x3110c0s7b0n0:1176:45958 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x148b76811c00
x3110c0s7b0n0:1176:45958 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x148b76812a00
x3110c0s7b0n0:1176:46045 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x148ac8000c60
x3110c0s7b0n0:1176:46045 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-9YKqm8

x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 0 from local rank 2, transport 2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Ring 00 : 0 -> 3 -> 2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Ring 01 : 0 -> 3 -> 2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1177:45953 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x149956210c00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x149956211a00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x149956211c00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x149956212a00
x3110c0s7b0n0:1177:46027 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x1498e0000c60
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy recv connection 0 from local rank 3, transport 0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000c80
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149903800000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000c80
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 00/0 : 2[85000] -> 5[46000] [send] via NET/Socket/0
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 1 from local rank 2, transport 2
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy recv connection 1 from local rank 3, transport 0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000cc0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 01/0 : 2[85000] -> 5[46000] [send] via NET/Socket/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/4/-1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Tree 1 : 4 -> 0 -> 1/-1/-1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 00/02 :    0   3   2   5   4   7   6   1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 01/02 :    0   3   2   5   4   7   6   1
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Ring 00 : 1 -> 0 -> 3
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Ring 01 : 1 -> 0 -> 3
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/-1/-1->0->4
x3110c0s7b0n0:1174:45954 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1174:45954 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14d036813200
x3110c0s7b0n0:1174:45954 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14d036814000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14d036814200
x3110c0s7b0n0:1174:45954 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14d036815000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000cc0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14cf54000c60
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149905600000
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000c80
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14cf77800000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Ring 00 : 6 -> 1 -> 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Ring 01 : 6 -> 1 -> 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1175:45961 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14f5de210c00
x3110c0s7b0n0:1175:45961 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14f5de211a00
x3110c0s7b0n0:1175:45961 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14f5de211c00
x3110c0s7b0n0:1175:45961 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14f5de212a00
x3110c0s7b0n0:1175:46041 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14f574000c60
x3110c0s7b0n0:1175:46041 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-tEOiU2

x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 0 from local rank 1, transport 2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 1 from local rank 0, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000c80
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 00/0 : 6[85000] -> 1[46000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000cc0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14cf79600000
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x148aeb600000
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x148aed200000
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 1 from local rank 1, transport 2
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000cc0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 01/0 : 6[85000] -> 1[46000] [receive] via NET/Socket/0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy send connection 2 from local rank 3, transport 0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy recv connection 2 from local rank 0, transport 0
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy recv connection 2 from local rank 2, transport 0
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14f591600000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000d00
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x149909600000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000d00
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148ae9800000
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14f593200000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy send connection 3 from local rank 3, transport 0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000d00
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14cf81600000
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy recv connection 3 from local rank 2, transport 0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000d40
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14990b600000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy send connection 2 from local rank 1, transport 0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000d40
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148aef600000
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy recv connection 3 from local rank 0, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000d00
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f58f800000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy recv connection 4 from local rank 3, transport 0
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000d80
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149913200000
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy recv connection 4 from local rank 2, transport 0
x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy recv connection 5 from local rank 3, transport 0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000d40
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14cf83600000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy send connection 3 from local rank 1, transport 0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000d80
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148af7600000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000d40
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f595600000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000dc0
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149913800000
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy recv connection 5 from local rank 2, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 0
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 4 from local rank 1, transport 0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000d80
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d042a00000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000d80
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f59d600000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000dc0
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148af9200000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 5 from local rank 0, transport 0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000dc0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14d04f800000
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 5 from local rank 1, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000dc0
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f59f200000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 6 from local rank 2, transport 0
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy send connection 6 from local rank 1, transport 0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000e00
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148af9800000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000e00
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f59f800000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy send connection 7 from local rank 1, transport 0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 7 from local rank 2, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000e40
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14f5a1600000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000e40
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148afb600000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1177:45953 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1177:45953 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1177:46027 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-xLNT79

x3110c0s7b0n0:1177:46027 [3] NCCL INFO New proxy send connection 6 from local rank 3, transport 2
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 8 from local rank 1, transport 0
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 8 from local rank 2, transport 0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-tDjfoX

x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy recv connection 6 from local rank 0, transport 2
x3110c0s7b0n0:1177:45953 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x1498e0000e00
x3110c0s7b0n0:1177:46027 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x149905c00000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000e00
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000e80
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f603800000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy recv connection 7 from local rank 0, transport 2
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000e80
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b45800000
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy recv connection 9 from local rank 1, transport 0
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000ec0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000e40
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 9 from local rank 2, transport 0
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f56e000000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000ec0
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x148b8f800000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 01/0 : 4[7000] -> 0[7000] [receive] via NET/Socket/0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 8 from local rank 0, transport 2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000e80
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/Socket/0
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1176:45958 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1176:45958 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1175:45961 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1175:45961 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1175:46041 [1] NCCL INFO New proxy send connection 10 from local rank 1, transport 2
x3110c0s7b0n0:1176:46045 [2] NCCL INFO New proxy send connection 10 from local rank 2, transport 2
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 9 from local rank 0, transport 2
x3110c0s7b0n0:1175:45961 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f574000f00
x3110c0s7b0n0:1175:46041 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14f56f200000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148ac8000f00
x3110c0s7b0n0:1176:46045 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x148ac6c00000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000ec0
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Channel 01/0 : 0[7000] -> 4[7000] [send] via NET/Socket/0
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14cf5ac00000
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14cf5b600000
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14cf4e000000
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14cf4ea00000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            512 |              0 |              0 |            512 |              0 |              0 |            512 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO     AllReduce |    20.0/   3.1 |    28.9/   0.0 |   224.0/  11.8 |    22.9/   3.4 |    45.0/   0.0 |    80.8/  13.7 |     7.4/   0.0 |     7.4/   0.0 |    29.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |    48.0/   0.0 |
x3110c0s7b0n0:1174:45954 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3110c0s7b0n0:1174:45954 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3110c0s7b0n0:1174:46040 [0] NCCL INFO New proxy send connection 10 from local rank 0, transport 2
x3110c0s7b0n0:1174:45954 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf54000f00
x3110c0s7b0n0:1174:45954 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14d036815200
x3110c0s7b0n0:1175:45961 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14f5de212c00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x149956212c00
x3110c0s7b0n0:1176:45958 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x148b76812c00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x1498de000000
x3110c0s7b0n0:1177:45953 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x149959800a00
x3110c0s7b0n0:1177:45953 [3] NCCL INFO comm 0x56257e51ebd0 rank 3 nranks 8 cudaDev 3 busId c7000 - Init COMPLETE
x3110c0s7b0n0:1176:45958 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x148ac4000000
x3110c0s7b0n0:1176:45958 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x148b75e00a00
x3110c0s7b0n0:1176:45958 [2] NCCL INFO comm 0x55577a4cb170 rank 2 nranks 8 cudaDev 2 busId 85000 - Init COMPLETE
x3110c0s7b0n0:1175:45961 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14f566000000
x3110c0s7b0n0:1175:45961 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14f5e1800a00
x3110c0s7b0n0:1175:45961 [1] NCCL INFO comm 0x55d66d14adf0 rank 1 nranks 8 cudaDev 1 busId 46000 - Init COMPLETE
x3110c0s7b0n0:1174:45954 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14cf4c000000
x3110c0s7b0n0:1174:45954 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14d035e00e00
x3110c0s7b0n0:1174:45954 [0] NCCL INFO comm 0x564802f2e850 rank 0 nranks 8 cudaDev 0 busId 7000 - Init COMPLETE
x3110c0s7b0n0:1174:46040 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14cf4f400000
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66d14adf0 [nranks=8] stream 0x55d66b8ae7c0
x3110c0s7b0n0:1175:1175 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e51ebd0 [nranks=8] stream 0x56257c69c120
x3110c0s7b0n0:1177:1177 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x564802f2e850 [nranks=8] stream 0x5648013dc580
x3110c0s7b0n0:1174:1174 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000048000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000018000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4cb170 [nranks=8] stream 0x5557788ade30
x3110c0s7b0n0:1176:1176 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 11 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 39644 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 39480 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 36716 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_800ns_512sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_800ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_800ns_512sl_1234s_shuffle_idx.npy
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66cc87de0 [nranks=1] stream 0x55d66b8ae700

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 1 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66d14adf0 [nranks=8] stream 0x55d66b8ae7c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4b5d50 [nranks=1] stream 0x5557788add70

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000047000 seconds
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 1 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4cb170 [nranks=8] stream 0x5557788ade30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e541070 [nranks=1] stream 0x56257c69bfe0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000036000 seconds
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 1 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e51ebd0 [nranks=8] stream 0x56257c69c120
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2    loaded indexed file in 0.037 seconds
    total number of samples: 40959
    total number of epochs: 1
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x5648029e40d0 [nranks=1] stream 0x5648013dc440

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000036000 seconds
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 1 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x564802f2e850 [nranks=8] stream 0x5648013dc580

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000028000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000017000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 51321 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 57968 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 32726 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_88ns_512sl_1234s_doc_idx.npy

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 69114 us
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_88ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_88ns_512sl_1234s_shuffle_idx.npy
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66cc87de0 [nranks=1] stream 0x55d66b8ae700

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 2 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 4 op 0 root 0 comm 0x55d66d14adf0 [nranks=8] stream 0x55d66b8ae7c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e541070 [nranks=1] stream 0x56257c69bfe0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000036000 seconds
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 2 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 4 op 0 root 0 comm 0x56257e51ebd0 [nranks=8] stream 0x56257c69c120
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4b5d50 [nranks=1] stream 0x5557788add70

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 2 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 4 op 0 root 0 comm 0x55577a4cb170 [nranks=8] stream 0x5557788ade30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2    loaded indexed file in 0.051 seconds
    total number of samples: 1406
    total number of epochs: 1
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x5648029e40d0 [nranks=1] stream 0x5648013dc440

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000036000 seconds
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 2 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 4 op 0 root 0 comm 0x564802f2e850 [nranks=8] stream 0x5648013dc580

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000031000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000019000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 4046 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_8ns_512sl_1234s_doc_idx.npy

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 78961 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 61202 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 92107 us
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_8ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_8ns_512sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 24
    total number of epochs: 1
> finished creating GPT datasets ...
x3110c0s7b0n0:1174:1174 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14d035e01000
x3110c0s7b0n0:1176:1176 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x148b75e00c00
x3110c0s7b0n0:1175:1175 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14f5e1800c00
x3110c0s7b0n0:1177:1177 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x149959800c00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Using network Socket
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Using network Socket
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Using network Socket
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Using network Socket
x3110c0s7b0n0:1174:46157 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1174:46157 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1174:46157 [0] NCCL INFO CPU/3 (1/2/-1)
x3110c0s7b0n0:1174:46157 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1174:46157 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3110c0s7b0n0:1174:46157 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3110c0s7b0n0:1174:46157 [0] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1174:46157 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3110c0s7b0n0:1174:46157 [0] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO ==========================================
x3110c0s7b0n0:1174:46157 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1175:46162 [1] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1175:46162 [1] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1175:46162 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3110c0s7b0n0:1175:46162 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (0)
x3110c0s7b0n0:1175:46162 [1] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO ==========================================
x3110c0s7b0n0:1175:46162 [1] NCCL INFO GPU/46000 :GPU/46000 (0/5000.000000/LOC) CPU/2 (2/24.000000/PHB) 
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1175:46162 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de214200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de214600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de214800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de214c00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de214e00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de215200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de215400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de215800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de215a00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de215e00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de216000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de216400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de216600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de216a00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de216c00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de217000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de217200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de217600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de217800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de217c00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de217e00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de218200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de218400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de218800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de218a00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de218e00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de219000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de219400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de219600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de219a00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de219c00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21a000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21a200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21a600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21a800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21ac00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21ae00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21b200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21b400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21b800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21ba00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21be00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21c000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21c400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21c600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21ca00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21cc00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21d000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21d200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21d600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21d800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21dc00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21de00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21e200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21e400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21e800
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21ea00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21ee00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21f000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21f400
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21f600
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de21fa00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14f5de21fc00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14f5de220000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Connected all rings
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1175:46162 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1174:46157 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036816800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036816c00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036816e00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036817200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036817400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036817800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036817a00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036817e00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036818000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036818400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036818600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036818a00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036818c00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036819000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036819200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036819600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036819800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036819c00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036819e00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681a200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681a400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681a800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681aa00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681ae00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681b000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681b400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681b600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681ba00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681bc00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681c000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681c200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681c600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681c800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681cc00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681ce00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681d200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681d400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681d800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681da00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681de00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681e000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681e400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681e600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681ea00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681ec00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681f000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681f200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681f600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681f800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d03681fc00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d03681fe00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036820200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036820400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036820800
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036820a00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036820e00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036821000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036821400
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036821600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036821a00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036821c00
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036822000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14d036822200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14d036822600
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Connected all rings
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1174:46157 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1177:46165 [3] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1177:46165 [3] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1177:46165 [3] NCCL INFO CPU/0 (1/2/-1)
x3110c0s7b0n0:1177:46165 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1177:46165 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3110c0s7b0n0:1177:46165 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (0)
x3110c0s7b0n0:1177:46165 [3] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1177:46165 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO ==========================================
x3110c0s7b0n0:1177:46165 [3] NCCL INFO GPU/C7000 :GPU/C7000 (0/5000.000000/LOC) CPU/0 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1177:46165 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956214200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956214600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956214800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956214c00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956214e00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956215200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956215400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956215800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956215a00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956215e00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956216000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956216400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956216600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956216a00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956216c00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956217000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956217200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956217600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956217800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956217c00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956217e00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956218200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956218400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956218800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956218a00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956218e00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956219000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956219400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956219600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956219a00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x149956219c00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621a000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621a200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621a600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621a800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621ac00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621ae00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621b200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621b400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621b800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621ba00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621be00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621c000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621c400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621c600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621ca00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621cc00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621d000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621d200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621d600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621d800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621dc00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621de00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621e200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621e400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621e800
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621ea00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621ee00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621f000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621f400
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621f600
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14995621fa00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14995621fc00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x149956220000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Connected all rings
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1177:46165 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1174:46181 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14cf48000cf0
x3110c0s7b0n0:1174:46181 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-90xBpX

x3110c0s7b0n0:1174:46181 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1176:46159 [2] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'hsn0'
x3110c0s7b0n0:1176:46159 [2] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3110c0s7b0n0:1176:46159 [2] NCCL INFO CPU/1 (1/2/-1)
x3110c0s7b0n0:1176:46159 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3110c0s7b0n0:1176:46159 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3110c0s7b0n0:1176:46159 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (0)
x3110c0s7b0n0:1176:46159 [2] NCCL INFO CPU/2 (1/2/-1)
x3110c0s7b0n0:1176:46159 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO + PCI[24.0] - NIC/43000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO ==========================================
x3110c0s7b0n0:1176:46159 [2] NCCL INFO GPU/85000 :GPU/85000 (0/5000.000000/LOC) CPU/1 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  0 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  1 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  2 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  3 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  4 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  5 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  6 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  7 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  8 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO  9 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 10 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 11 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 12 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 13 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 14 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 15 : GPU/0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 00/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 01/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 02/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 03/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 04/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 05/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 06/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 07/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 08/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 09/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 10/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 11/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 12/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 13/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 14/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 15/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 16/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 17/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 18/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 19/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 20/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 21/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 22/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 23/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 24/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 25/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 26/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 27/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 28/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 29/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 30/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Channel 31/32 :    0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3110c0s7b0n0:1176:46159 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76814200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76814600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76814800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76814c00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76814e00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76815200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76815400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76815800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76815a00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76815e00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76816000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76816400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76816600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76816a00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76816c00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76817000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76817200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76817600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76817800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76817c00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76817e00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76818200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76818400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76818800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76818a00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76818e00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76819000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76819400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76819600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76819a00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b76819c00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681a000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681a200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681a600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681a800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681ac00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681ae00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681b200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681b400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681b800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681ba00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681be00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681c000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681c400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681c600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681ca00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681cc00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681d000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681d200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681d600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681d800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681dc00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681de00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681e200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681e400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681e800
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681ea00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681ee00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681f000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681f400
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681f600
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b7681fa00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x148b7681fc00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x148b76820000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Connected all rings
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Connected all trees by HAO
x3110c0s7b0n0:1176:46159 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3110c0s7b0n0:1174:46157 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cf48000d10
x3110c0s7b0n0:1174:46157 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14d036822800
x3110c0s7b0n0:1175:46183 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14f558000cf0
x3110c0s7b0n0:1175:46183 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-cCupf5

x3110c0s7b0n0:1175:46183 [1] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1174:46181 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14cf38000000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14cf42000000
x3110c0s7b0n0:1174:46157 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14d035e01200
x3110c0s7b0n0:1174:46157 [0] NCCL INFO comm 0x564803518fb0 rank 0 nranks 1 cudaDev 0 busId 7000 - Init COMPLETE
x3110c0s7b0n0:1176:46190 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x148abc000cf0
x3110c0s7b0n0:1176:46190 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-uT8Dx6

x3110c0s7b0n0:1176:46190 [2] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3110c0s7b0n0:1177:46185 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x1498d8000cf0
x3110c0s7b0n0:1177:46185 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-aYTGJb

x3110c0s7b0n0:1177:46185 [3] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 24 datatype 0 op 0 root 0 comm 0x564803518fb0 [nranks=1] stream 0x5648010a2e60
x3110c0s7b0n0:1175:46162 [1] NCCL INFO Connection to proxy localRank 0 -> connection 0x14f558000d10
x3110c0s7b0n0:1175:46162 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14f5de220200
x3110c0s7b0n0:1176:46159 [2] NCCL INFO Connection to proxy localRank 0 -> connection 0x148abc000d10
x3110c0s7b0n0:1176:46159 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x148b76820200
x3110c0s7b0n0:1177:46165 [3] NCCL INFO Connection to proxy localRank 0 -> connection 0x1498d8000d10
x3110c0s7b0n0:1177:46165 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x149956220200
x3110c0s7b0n0:1175:46162 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14f556000000
x3110c0s7b0n0:1175:46162 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14f5e1800e00
x3110c0s7b0n0:1175:46162 [1] NCCL INFO comm 0x55d66d8f5610 rank 0 nranks 1 cudaDev 1 busId 46000 - Init COMPLETE
x3110c0s7b0n0:1175:46183 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14f552000000
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 1 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3110c0s7b0n0:1175:1175 [1] NCCL INFO Broadcast: opCount 0 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 24 datatype 0 op 0 root 0 comm 0x55d66d8f5610 [nranks=1] stream 0x55d66b88cd40
x3110c0s7b0n0:1176:46190 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x148ab0000000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x148aba000000
x3110c0s7b0n0:1176:46159 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x148b75e00e00
x3110c0s7b0n0:1176:46159 [2] NCCL INFO comm 0x55577a9bc3d0 rank 0 nranks 1 cudaDev 2 busId 85000 - Init COMPLETE
x3110c0s7b0n0:1177:46185 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x1498c8000000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x1498d6000000
x3110c0s7b0n0:1177:46165 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x149959800e00
x3110c0s7b0n0:1177:46165 [3] NCCL INFO comm 0x56257e54cb40 rank 0 nranks 1 cudaDev 3 busId c7000 - Init COMPLETE
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 2 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3110c0s7b0n0:1176:1176 [2] NCCL INFO Broadcast: opCount 0 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 24 datatype 0 op 0 root 0 comm 0x55577a9bc3d0 [nranks=1] stream 0x55577888c330
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 3 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3110c0s7b0n0:1177:1177 [3] NCCL INFO Broadcast: opCount 0 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 24 datatype 0 op 0 root 0 comm 0x56257e54cb40 [nranks=1] stream 0x56257c69c260
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 7 sendbuff 0x14f62b228800 recvbuff 0x14f62b228800 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 7 sendbuff 0x14d043228800 recvbuff 0x14d043228800 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000053000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000021000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043228800
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043228800
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 7 sendbuff 0x1499a7228800 recvbuff 0x1499a7228800 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 7 sendbuff 0x148b83228800 recvbuff 0x148b83228800 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 375096 us
x3110c0s7b0n0:1175:1175 [1] NCCL INFO AllReduce: opCount 8 sendbuff 0x14f62b200400 recvbuff 0x14f62b200400 count 1 datatype 1 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 318801 us
x3110c0s7b0n0:1177:1177 [3] NCCL INFO AllReduce: opCount 8 sendbuff 0x1499a7200400 recvbuff 0x1499a7200400 count 1 datatype 1 op 0 root 0 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 295853 us
x3110c0s7b0n0:1176:1176 [2] NCCL INFO AllReduce: opCount 8 sendbuff 0x148b83200400 recvbuff 0x148b83200400 count 1 datatype 1 op 0 root 0 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 359561 us
[after dataloaders are built] datetime: 2023-12-19 01:30:35 
done with setup ...
training ...
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO AllReduce: opCount 8 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 1 datatype 1 op 0 root 0 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000039000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000019000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14d043200400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 159759 us
before losses_reduced = forward_backward_func

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 162425 us
before losses_reduced = forward_backward_func

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 142485 us
before losses_reduced = forward_backward_func

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 125618 us
[before the start of training step] datetime: 2023-12-19 01:30:35 
before losses_reduced = forward_backward_func
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 5, datatype: ncclInt64, message size: 40 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14d043200400 recvbuff 0x14d043200400 count 40 datatype 0 op 0 root 0 comm 0x564803518fb0 [nranks=1] stream 0x5648010a2e60
[Hao INFO in ncclRecv (sendrecv.cc)] rank 3 Recv: count: 4915200, datatype: ncclFloat16/ncclHalf, message size: 9830400 Bytes
x3110c0s7b0n0:1177:1177 [3] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x14990d200000 count 4915200 datatype 6 op 0 root 2 comm 0x56257d333060 [nranks=8] stream 0x56257c69bea0
[Hao INFO in ncclRecv (sendrecv.cc)] rank 1 Recv: count: 4915200, datatype: ncclFloat16/ncclHalf, message size: 9830400 Bytes
x3110c0s7b0n0:1175:1175 [1] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x14f599200000 count 4915200 datatype 6 op 0 root 0 comm 0x55d66c5036d0 [nranks=8] stream 0x55d66b8ae5c0
[Hao INFO in ncclRecv (sendrecv.cc)] rank 2 Recv: count: 4915200, datatype: ncclFloat16/ncclHalf, message size: 9830400 Bytes
x3110c0s7b0n0:1176:1176 [2] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x148af3200000 count 4915200 datatype 6 op 0 root 1 comm 0x555779502ba0 [nranks=8] stream 0x5557788adc30
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 11 from local rank 2, transport 0
x3110c0s7b0n0:1176:47249 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000f40
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 7 from local rank 3, transport 0
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 11 from local rank 1, transport 0
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 4104, datatype: ncclInt64, message size: 32832 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14d043281e00 recvbuff 0x14d043281e00 count 32832 datatype 0 op 0 root 0 comm 0x564803518fb0 [nranks=1] stream 0x5648010a2e60
x3110c0s7b0n0:1177:47247 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000e40
x3110c0s7b0n0:1175:47248 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000f40
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f56f600000
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149981600000
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148ac7000000
x3110c0s7b0n0:1176:44672 [2] NCCL INFO New proxy recv connection 12 from local rank 2, transport 0
x3110c0s7b0n0:1176:47249 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x148b1c000f80
x3110c0s7b0n0:1177:44676 [3] NCCL INFO New proxy recv connection 8 from local rank 3, transport 0
x3110c0s7b0n0:1177:47247 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x149948000e80
x3110c0s7b0n0:1177:44676 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x149983600000
x3110c0s7b0n0:1175:44681 [1] NCCL INFO New proxy recv connection 12 from local rank 1, transport 0
x3110c0s7b0n0:1175:47248 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14f5d0000f80
x3110c0s7b0n0:1176:44672 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x148ac7600000
x3110c0s7b0n0:1175:44681 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14f52a000000
before timers('forward-send').start()
before _communicate(
[Hao INFO in ncclSend (sendrecv.cc)] rank 0 Send: count: 4915200, datatype: ncclFloat16/ncclHalf, message size: 9830400 Bytes
x3110c0s7b0n0:1174:1174 [0] NCCL INFO Send: opCount 9 sendbuff (nil) recvbuff 0x14ce5d770000 count 4915200 datatype 6 op 0 root 1 comm 0x564802052d90 [nranks=8] stream 0x5648013dc200
x3110c0s7b0n0:1174:47484 [0] NCCL INFO Channel 00/1 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 11 from local rank 0, transport 0
x3110c0s7b0n0:1174:47484 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000f40
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ce3e600000
x3110c0s7b0n0:1174:47484 [0] NCCL INFO Channel 01/1 : 0[7000] -> 1[46000] via P2P/IPC/read
x3110c0s7b0n0:1174:44689 [0] NCCL INFO New proxy send connection 12 from local rank 0, transport 0
x3110c0s7b0n0:1174:47484 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14cfe4000f80
x3110c0s7b0n0:1174:44689 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ce3ec00000
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2