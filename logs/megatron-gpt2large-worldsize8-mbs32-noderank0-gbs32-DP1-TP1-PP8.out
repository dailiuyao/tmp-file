using world size: 8, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
using torch.float16 for parameters ...
Gradient accumulation fusion to linear layer weight gradient computation is supported only with fp32 gradient accumulation. Setting gradient_accumulation_fusion to False
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/home/yuke/lyd/Megatron-LM/my-gpt2_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_seq_length .............................. 512
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 4800
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  gradient_accumulation_fusion .................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1200
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 50
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /local/scratch/checkpoints/gpt2_774m
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 64
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 512
  merge_file ...................................... /home/yuke/lyd/Megatron-LM/model/gpt2-merges.txt
  micro_batch_size ................................ 32
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 24
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 40
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 8
  pipeline_model_parallel_split_rank .............. None
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /local/scratch/checkpoints/gpt2_774m
  save_interval ................................... 50
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 512
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 100
  train_samples ................................... None
  transformer_pipeline_model_parallel_size ........ 8
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /home/yuke/lyd/Megatron-LM/model/gpt2-vocab.json
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 8
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory '/home/yuke/lyd/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/yuke/lyd/Megatron-LM/megatron/data'
>>> done with dataset index builder. Compilation time: 1.231 seconds
> compiling and loading fused kernels ...
[1/3] c++ -MMD -MF scaled_upper_triang_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o scaled_upper_triang_masked_softmax.o 
[2/3] /soft/compilers/cudatoolkit/cuda-11.7.1/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -o scaled_upper_triang_masked_softmax_cuda.cuda.o 
[3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.7.1/lib64 -lcudart -o scaled_upper_triang_masked_softmax_cuda.so
[1/3] c++ -MMD -MF scaled_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax.cpp -o scaled_masked_softmax.o 
[2/3] /soft/compilers/cudatoolkit/cuda-11.7.1/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -o scaled_masked_softmax_cuda.cuda.o 
[3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.7.1/lib64 -lcudart -o scaled_masked_softmax_cuda.so
[1/3] c++ -MMD -MF scaled_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_softmax.cpp -o scaled_softmax.o 
[2/3] /soft/compilers/cudatoolkit/cuda-11.7.1/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/scaled_softmax_cuda.cu -o scaled_softmax_cuda.cuda.o 
[3/3] c++ scaled_softmax.o scaled_softmax_cuda.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.7.1/lib64 -lcudart -o scaled_softmax_cuda.so
[1/3] c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda.cpp -o layer_norm_cuda.o 
[2/3] /soft/compilers/cudatoolkit/cuda-11.7.1/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/TH -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/THC -isystem /soft/compilers/cudatoolkit/cuda-11.7.1/include -isystem /home/yuke/lyd/conda3/envs/pytorchNCCL-hao/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -maxrregcount=50 -gencode arch=compute_80,code=sm_80 -std=c++17 -c /home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o 
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function 'void cuda_layer_norm(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double)':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:223: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                               ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:246: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:271: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                               ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:295: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:358: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:413: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                             ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:118: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:141: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                             ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:166: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:190: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:257: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:316: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                            ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:130: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:153: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:178: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:202: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:273: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:336: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:150: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:173: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                             ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:198: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:226: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:293: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:352: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:165: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:188: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                            ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:213: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:245: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:316: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                            ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:706:379: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  706 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                           ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function 'void cuda_layer_norm_gradient(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double, at::Tensor*, at::Tensor*, at::Tensor*)':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:223: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                               ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:246: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:271: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                               ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:332: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                            ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:388: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:437: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:488: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:549: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:119: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:142: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:167: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:232: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                        ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:292: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:341: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                     ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:396: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                            ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:461: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:131: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                   ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:154: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:179: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                   ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:248: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                        ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:312: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                        ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:361: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:420: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:489: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:151: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:199: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:264: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                        ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:324: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:377: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:432: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:497: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:166: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:189: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                             ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:214: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                      ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:283: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                           ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:347: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                           ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:404: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:463: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:532: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  814 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = float]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:203:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::Half]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:95:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::BFloat16]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:103:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::Half; U = float; V = c10::Half]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:127:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = c10::Half]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of 'void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::BFloat16; U = float; V = c10::BFloat16]':
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:814:138:   required from here
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:138: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                          ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:210: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                  ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:751:247: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  751 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
      |                                                                                                                                                                                                                                                       ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:137: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                         ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:764:174: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  764 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
      |                                                                                                                                                                              ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
/home/yuke/lyd/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu:782:129: warning: 'T* at::Tensor::data() const [with T = c10::BFloat16]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
  782 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
      |                                                                                                                                 ^ 
/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:244:1: note: declared here
  244 |   T * data() const {
      | ^ ~~
[3/3] c++ layer_norm_cuda.o layer_norm_cuda_kernel.cuda.o -shared -L/home/yuke/lyd/conda3/envs/pytorchNCCL-hao/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/soft/compilers/cudatoolkit/cuda-11.7.1/lib64 -lcudart -o fused_mix_prec_layer_norm_cuda.so
x3002c0s19b1n0:19714:19714 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19714:19714 [0] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3002c0s19b1n0:19714:19714 [0] NCCL INFO Bootstrap : Using hsn0:10.201.1.95<0>
x3002c0s19b1n0:19714:19714 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3002c0s19b1n0:19717:19717 [3] NCCL INFO cudaDriverVersion 11040
x3002c0s19b1n0:19715:19715 [1] NCCL INFO cudaDriverVersion 11040
x3002c0s19b1n0:19716:19716 [2] NCCL INFO cudaDriverVersion 11040
x3002c0s19b1n0:19714:19714 [0] NCCL INFO cudaDriverVersion 11040
NCCL version 2.14.3+cuda11.8
x3002c0s19b1n0:19714:19714 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14ac7b000000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB hsn0:10.201.1.95<0>
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Using network IB
x3002c0s19b1n0:19715:19715 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19715:19715 [1] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3002c0s19b1n0:19715:19715 [1] NCCL INFO Bootstrap : Using hsn0:10.201.1.95<0>
x3002c0s19b1n0:19715:19715 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3002c0s19b1n0:19715:19715 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14fe4f400000
x3002c0s19b1n0:19716:19716 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19716:19716 [2] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3002c0s19b1n0:19716:19716 [2] NCCL INFO Bootstrap : Using hsn0:10.201.1.95<0>
x3002c0s19b1n0:19716:19716 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3002c0s19b1n0:19716:19716 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x15160ce00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19717:19717 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19717:19717 [3] NCCL INFO NCCL_SOCKET_IFNAME set to hsn0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB hsn0:10.201.1.95<0>
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Using network IB
x3002c0s19b1n0:19717:19717 [3] NCCL INFO Bootstrap : Using hsn0:10.201.1.95<0>
x3002c0s19b1n0:19717:19717 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
x3002c0s19b1n0:19717:19717 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14a152e00000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB hsn0:10.201.1.95<0>
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Using network IB
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB hsn0:10.201.1.95<0>
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Using network IB
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19715:63386 [1] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x14fe4fe00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19715:63386 [1] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO ==========================================
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19714:63382 [0] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x14ac7ba00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19714:63382 [0] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO ==========================================
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19717:63391 [3] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x14a153800000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19717:63391 [3] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO ==========================================
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19716:63387 [2] NCCL INFO transport/p2p.cc:151 Cuda Alloc Size 2097152 pointer 0x15160d800000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19716:63387 [2] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO ==========================================
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/4/-1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Tree 2 : 4 -> 0 -> 1/-1/-1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/4/-1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Tree 3 : 4 -> 0 -> 1/-1/-1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 00/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 01/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 02/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 03/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Ring 00 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Ring 01 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Ring 02 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Ring 03 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x3002c0s19b1n0:19714:63382 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba00e00
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba01000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba01e00
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba02000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba02e00
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba03000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba03e00
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 00/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14ac6c000c60
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000c80
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac72000000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Tree 2 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Tree 3 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Ring 00 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Ring 01 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Ring 02 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Ring 03 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe00e00
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe01000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe01e00
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe02000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe02e00
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe03000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe03e00
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:63481 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14fe04000c60
x3002c0s19b1n0:19715:63481 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-K073sZ

x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 0 from local rank 1, transport 2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Ring 00 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Ring 01 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Ring 02 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Ring 03 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
x3002c0s19b1n0:19716:63387 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d800000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d800e00
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d801000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d801e00
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d802000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d802e00
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d803000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d803e00
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Ring 00 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Ring 01 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Ring 02 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Ring 03 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153800000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153800e00
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153801000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153801e00
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153802000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153802e00
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153803000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153803e00
x3002c0s19b1n0:19716:63485 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x151608000c60
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 0 from local rank 2, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14a144000c60
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 0 from local rank 3, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 01/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 1 from local rank 0, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000c80
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x15160da00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000c80
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000c80
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a153a00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000cc0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac72600000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 00/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 1 from local rank 1, transport 2
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 1 from local rank 3, transport 0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 1 from local rank 2, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000cc0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x151602000000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000cc0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 01/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 2 from local rank 1, transport 2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000cc0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a14a000000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000d00
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000d00
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac72c00000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 2 from local rank 2, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 2 from local rank 3, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 02/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/1
x3002c0s19b1n0:19715:63386 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 3 from local rank 1, transport 2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000d40
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000d00
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x151602600000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000d00
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a14a600000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 3 from local rank 2, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000d40
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac73200000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 03/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 3 from local rank 3, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000d40
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x151602c00000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000d40
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a14ac00000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 4 from local rank 2, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-Da7Kg6

x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 4 from local rank 3, transport 2
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000d80
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 5[46000] [send] via NET/IB/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 5 from local rank 3, transport 2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000d80
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151600c00000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fe0401d000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fe04024000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000dc0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 5[46000] [send] via NET/IB/0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 6 from local rank 3, transport 2
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fe0404b000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fe50000000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14fe040403c0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 5 from local rank 2, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000e00
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 5[46000] [send] via NET/IB/1
x3002c0s19b1n0:19717:63391 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000dc0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151601200000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fe0404d000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fe04054000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fe04079000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fe50a00000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14fe0406e4f0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fe0407b000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fe04082000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 7 from local rank 3, transport 2
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fe0409d000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fe51400000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 6 from local rank 2, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fe0409f000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fe040a6000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fe040c1000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fe51e00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 4 from local rank 1, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000e00
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151601800000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 7 from local rank 2, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000e40
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 5[46000] [send] via NET/IB/0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a144020000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000e40
x3002c0s19b1n0:19717:63484 [3] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6980 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a144042000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a142c00000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14a14402f0a0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000d80
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe52800000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fe000000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connected all rings
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 8 from local rank 2, transport 0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 4 from local rank 0, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a144044000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000e80
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fc000000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6712 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a144065000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a143600000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14a1440521f0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 9 from local rank 2, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 5 from local rank 1, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000d80
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14ac6b200000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a144067000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6981 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a14407e000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a13a000000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000ec0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fc600000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000dc0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe52e00000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 5 from local rank 0, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000dc0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14ac6b800000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 10 from local rank 2, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000f00
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 6 from local rank 1, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a144080000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fcc00000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6713 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a144097000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a13aa00000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 6 from local rank 0, transport 0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 11 from local rank 2, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000e00
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe53400000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connected all rings
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 8 from local rank 3, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000e80
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a13b400000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000e00
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14ac68000000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000f40
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fd200000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 7 from local rank 1, transport 0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 7 from local rank 0, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 9 from local rank 3, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000e40
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe53a00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000e40
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14ac68600000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000ec0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a13ba00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connected all rings
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 10 from local rank 3, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connected all rings
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 8 from local rank 1, transport 0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 8 from local rank 0, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000f00
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a138000000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000e80
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac66600000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000e80
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe55800000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 9 from local rank 0, transport 0
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 11 from local rank 3, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 9 from local rank 1, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000ec0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac66c00000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000f40
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a138600000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000ec0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe55e00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 10 from local rank 0, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 10 from local rank 1, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000f00
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac67200000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000f00
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe56400000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 11 from local rank 0, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 11 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000f40
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000f40
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14ac67800000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe56a00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 12 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000f80
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe57000000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 13 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04000fc0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe57600000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 14 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe57c00000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 15 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001040
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fe58200000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 12 from local rank 2, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000f80
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-kn6FgS

x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 12 from local rank 0, transport 2
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 16 from local rank 1, transport 0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fb200000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000f80
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001080
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe5b800000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 13 from local rank 0, transport 2
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 13 from local rank 2, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c000fc0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 17 from local rank 1, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608000fc0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515fb800000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 01/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 14 from local rank 0, transport 2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe040010c0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 14 from local rank 2, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe5be00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001000
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515f8000000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 18 from local rank 1, transport 0
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001100
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe5c400000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy recv connection 15 from local rank 0, transport 2
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 15 from local rank 2, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 19 from local rank 1, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001040
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515f8600000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001140
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe5ca00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001040
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 16 from local rank 2, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001080
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515f8c00000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 17 from local rank 2, transport 0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x1516080010c0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515f9200000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 03/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 16 from local rank 0, transport 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001080
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 17 from local rank 0, transport 2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 18 from local rank 2, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c0010c0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 01/0 : 0[7000] -> 4[7000] [send] via NET/IB/0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 18 from local rank 0, transport 2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001100
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515f9800000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 19 from local rank 2, transport 0
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001100
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/1
x3002c0s19b1n0:19714:63382 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 19 from local rank 0, transport 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001140
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Channel 03/0 : 0[7000] -> 4[7000] [send] via NET/IB/0
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001140
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515f6000000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19716:63387 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19716:63387 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19716:63485 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-0wHFn3

x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy send connection 20 from local rank 2, transport 2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19715:63386 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19715:63386 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy send connection 20 from local rank 1, transport 2
x3002c0s19b1n0:19715:63386 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001180
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19717:63391 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19717:63391 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy send connection 12 from local rank 3, transport 2
x3002c0s19b1n0:19716:63387 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001180
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x1515f5800000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000f80
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14a136600000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14fe5e800000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac6c03a000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6982 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac6c05c000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac5e000000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14ac6c049460
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac6c05e000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6714 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac6c07f000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac5ea00000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO Mem Realloc old size 0, new size 768 pointer 0x14ac6c06c840
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac6c081000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6983 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac6c098000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac5f400000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac6c09a000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6715 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac6c0b1000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac5c000000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac6c0b3000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac6c0ba000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac6c0d6000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac5ca00000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac6c0d8000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac6c0df000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac6c0fa000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac5d400000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac6c0fc000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac6c103000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac6c11e000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac5a000000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac6c120000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac6c127000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac6c142000
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac5aa00000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            512 |              0 |              0 |            512 |              0 |              0 |            512 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO     AllReduce |    20.0/   5.8 |    28.9/   0.0 |   224.0/  22.1 |    22.9/   3.4 |    45.0/   0.0 |    80.8/  13.7 |     7.4/   0.0 |     7.4/   0.0 |    29.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |    48.0/   0.0 |
x3002c0s19b1n0:19714:63382 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19714:63382 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 20 from local rank 0, transport 2
x3002c0s19b1n0:19714:63382 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001180
x3002c0s19b1n0:19714:63382 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14ac7ba04000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14fe4fe04000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14a153804000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x15160d804000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14a134000000
x3002c0s19b1n0:19717:63391 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14a152e00200
x3002c0s19b1n0:19717:63391 [3] NCCL INFO comm 0x561b4ac019d0 rank 3 nranks 8 cudaDev 3 busId c7000 - Init COMPLETE
x3002c0s19b1n0:19716:63387 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x1515f2000000
x3002c0s19b1n0:19716:63387 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x15160ce00200
x3002c0s19b1n0:19716:63387 [2] NCCL INFO comm 0x557f892b8060 rank 2 nranks 8 cudaDev 2 busId 85000 - Init COMPLETE
x3002c0s19b1n0:19715:63386 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14fe5f000000
x3002c0s19b1n0:19715:63386 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14fe4f400200
x3002c0s19b1n0:19715:63386 [1] NCCL INFO comm 0x55b1d39c5440 rank 1 nranks 8 cudaDev 1 busId 46000 - Init COMPLETE
x3002c0s19b1n0:19714:63382 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14ac58000000
x3002c0s19b1n0:19714:63382 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14ac7b000200
x3002c0s19b1n0:19714:63382 [0] NCCL INFO comm 0x562dd1e1f3a0 rank 0 nranks 8 cudaDev 0 busId 7000 - Init COMPLETE
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14ac5b400000
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14fe4ba00200 recvbuff 0x14fe4ba00200 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
x3002c0s19b1n0:19715:19715 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14acbb600200 recvbuff 0x14acbb600200 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0
x3002c0s19b1n0:19714:19714 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000042000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600200
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600200
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x15164f600200 recvbuff 0x15164f600200 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
x3002c0s19b1n0:19716:19716 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x14a193600200 recvbuff 0x14a193600200 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
x3002c0s19b1n0:19717:19717 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 310812 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 288272 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 1 sendbuff 0x14acbb600000 recvbuff 0x14acbb600000 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000034000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000013000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 285667 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 274981 us
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 1 sendbuff 0x14fe4ba00000 recvbuff 0x14fe4ba00000 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 1 sendbuff 0x15164f600000 recvbuff 0x15164f600000 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 1 sendbuff 0x14a193600000 recvbuff 0x14a193600000 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 9 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 2181576 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 5835662 us
>>> done with compiling and loading fused kernels. Compilation time: 415.026 seconds

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 72418 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclFloat64/ncclDouble, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 2 sendbuff 0x14acbb600000 recvbuff 0x14acbb600000 count 1 datatype 8 op 3 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000054000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 2 sendbuff 0x14a193600000 recvbuff 0x14a193600000 count 1 datatype 8 op 3 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 2 sendbuff 0x14fe4ba00000 recvbuff 0x14fe4ba00000 count 1 datatype 8 op 3 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_FUNC_NAME (common.h)] 3 AllReduce TREE LL Min double Elapsed time: 4 us
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 2 sendbuff 0x15164f600000 recvbuff 0x15164f600000 count 1 datatype 8 op 3 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_FUNC_NAME (common.h)] 2 AllReduce TREE LL Min double Elapsed time: 6 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum double Elapsed time: 130 us
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 3 sendbuff 0x15164f600200 recvbuff 0x15164f600200 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_FUNC_NAME (common.h)] 0 AllReduce TREE LL Min double Elapsed time: 24749 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum double Elapsed time: 24846 us
time to initialize megatron (seconds): 421.725
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 3 sendbuff 0x14acbb600200 recvbuff 0x14acbb600200 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000024000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000012000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600200
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600200
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum double Elapsed time: 19372 us
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 3 sendbuff 0x14a193600200 recvbuff 0x14a193600200 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_FUNC_NAME (common.h)] 1 AllReduce TREE LL Min double Elapsed time: 17394 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum double Elapsed time: 17489 us
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 3 sendbuff 0x14fe4ba00200 recvbuff 0x14fe4ba00200 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 40663 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 48479 us
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 86478000

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 24292 us
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 86478000

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 40990 us
[after megatron is initialized] datetime: 2023-11-08 02:01:40 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 86478000
x3002c0s19b1n0:19714:19714 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14ac7b000400
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 4 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 4 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 4 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19714:65049 [0] NCCL INFO Using network IB
x3002c0s19b1n0:19714:65049 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19714:65049 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO === System : maxBw 12.0 totalBw 24.0 ===
x3002c0s19b1n0:19714:65049 [0] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19714:65049 [0] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19714:65049 [0] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO ==========================================
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) NET/0 (5/12.000000/SYS) NET/1 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65049 [0] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) CPU/2 (3/12.000000/SYS) NET/0 (0/5000.000000/LOC) NET/1 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65049 [0] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) NET/0 (5/12.000000/SYS) NET/1 (0/5000.000000/LOC) 
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type LOC/SYS, sameChannels 1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO  0 : NET/0 GPU/0 NET/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO  1 : NET/1 GPU/0 NET/1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 2, bw 24.000000/12.000000, type LOC/SYS, sameChannels 1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO  0 : NET/0 GPU/0 NET/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO  1 : NET/1 GPU/0 NET/1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Tree 1 : 1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 00/02 :    0   1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 01/02 :    0   1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Ring 00 : 1 -> 0 -> 1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Ring 01 : 1 -> 0 -> 1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
x3002c0s19b1n0:19714:65049 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19714:65049 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x14ac7ba05600
x3002c0s19b1n0:19714:65049 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x14ac7ba05c00
x3002c0s19b1n0:19714:65049 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 1152 pointer 0x14ac7ba05e00
x3002c0s19b1n0:19714:65049 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 8 pointer 0x14ac7ba06400
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14ac34000d10
x3002c0s19b1n0:19714:65050 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-JtXK8Q

x3002c0s19b1n0:19714:65050 [0] NCCL INFO New proxy recv connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac34000d30
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 00/0 : 1[c7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO New proxy recv connection 1 from local rank 0, transport 2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac34000d70
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 01/0 : 1[c7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac34000db0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[c7000] [send] via NET/IB/0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac34000df0
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[c7000] [send] via NET/IB/0
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac3401d000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6720 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac34035000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac9ac00000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14ac34037000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6721 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14ac3404e000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14ac9b600000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac34050000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac34057000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac34073000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac9c000000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14ac34075000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14ac3407c000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14ac34097000
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14ac9ca00000
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connected all rings
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            256 |              0 |              0 |            512 |              0 |              0 |            512 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   3.0 |    14.0/   0.0 |    18.0/  12.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   3.0 |    14.0/   0.0 |    18.0/  12.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO     AllReduce |    14.4/   2.9 |    21.4/   0.0 |    56.0/  11.0 |    10.8/   3.0 |    21.0/   0.0 |    35.4/  12.0 |     4.4/   0.0 |     4.4/   0.0 |    10.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65049 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
x3002c0s19b1n0:19714:65049 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19714:65050 [0] NCCL INFO New proxy send connection 4 from local rank 0, transport 2
x3002c0s19b1n0:19714:65049 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac34000e30
x3002c0s19b1n0:19714:65049 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14ac7ba06600
x3002c0s19b1n0:19714:65050 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 4194304 pointer 0x14ac57c00000
x3002c0s19b1n0:19714:65049 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14ac9d400000
x3002c0s19b1n0:19714:65049 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14ac7b000600
x3002c0s19b1n0:19714:65049 [0] NCCL INFO comm 0x562df3da9980 rank 0 nranks 2 cudaDev 0 busId 7000 - Init COMPLETE
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 60364800, datatype: ncclFloat16/ncclHalf, message size: 120729600 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14ac88000000 recvbuff 0x14ac88000000 count 60364800 datatype 6 op 0 root 0 comm 0x562df3da9980 [nranks=2] stream 0x562df0fbf8e0
x3002c0s19b1n0:19714:19714 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000048000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14ac88000000
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14ac88000000
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 60364800
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 6
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 288
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 120729600
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 2 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 147457200

[LYD INFO in NCCL_FUNC_NAME (common.h)] 0 AllReduce RING SIMPLE Sum half Elapsed time: 360683 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce RING LL Sum half Elapsed time: 360781 us
> learning rate decay style: cosine
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 4 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000031000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000013000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 7937 us
WARNING: could not find the metadata file /local/scratch/checkpoints/gpt2_774m/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 5 sendbuff 0x14acbb628800 recvbuff 0x14acbb628800 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000024000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000013000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb628800
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb628800
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 1861299 us
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 5 sendbuff 0x15164f628800 recvbuff 0x15164f628800 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 1914283 us
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 5 sendbuff 0x14fe4ba28800 recvbuff 0x14fe4ba28800 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 1908136 us
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 5 sendbuff 0x14a193628800 recvbuff 0x14a193628800 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 10 us
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 6 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 133606 us
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 6 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 137169 us
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 6 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 138724 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 6 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000027000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000013000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 56487 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 44889 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 12773 us
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-11-08 02:01:42 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      3200
    validation: 352
    test:       32
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
NCCL version 2.14.3+cuda11.8
x3002c0s19b1n0:19717:19717 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14a152e00400
NCCL version 2.14.3+cuda11.8
x3002c0s19b1n0:19715:19715 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14fe4f400400
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.042143 seconds
    number of documents: 5263
 > dataset split:
    train:
     document indices in [0, 5100) total of 5100 documents
    validation:
     document indices in [5100, 5258) total of 158 documents
    test:
     document indices in [5258, 5263) total of 5 documents
x3002c0s19b1n0:19714:19714 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14ac7b000800

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 32637 us
NCCL version 2.14.3+cuda11.8
x3002c0s19b1n0:19716:19716 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x15160ce00400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Using network IB
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Using network IB
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Using network IB
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Using network IB
x3002c0s19b1n0:19717:65285 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19717:65285 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19717:65285 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19717:65285 [3] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:65285 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (0)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO ==========================================
x3002c0s19b1n0:19717:65285 [3] NCCL INFO GPU/C7000 :GPU/C7000 (0/5000.000000/LOC) CPU/0 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19717:65285 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153805600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153805a00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153805c00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153806000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153806200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153806600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153806800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153806c00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153806e00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153807200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153807400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153807800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153807a00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153807e00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153808000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153808400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153808600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153808a00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153808c00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153809000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153809200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153809600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153809800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153809c00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153809e00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380a200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380a400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380a800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380aa00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380ae00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380b000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380b400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380b600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380ba00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380bc00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380c000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380c200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380c600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380c800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380cc00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380ce00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380d200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380d400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380d800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380da00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380de00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380e000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380e400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380e600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380ea00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380ec00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380f000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380f200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380f600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380f800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15380fc00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15380fe00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153810200
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153810400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153810800
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153810a00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153810e00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153811000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153811400
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Connected all rings
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19717:65285 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19715:65287 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19715:65287 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19715:65287 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19715:65287 [1] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (0)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:65287 [1] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO ==========================================
x3002c0s19b1n0:19715:65287 [1] NCCL INFO GPU/46000 :GPU/46000 (0/5000.000000/LOC) CPU/2 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) 
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19715:65287 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe05600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe05a00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe05c00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe06000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe06200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe06600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe06800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe06c00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe06e00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe07200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe07400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe07800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe07a00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe07e00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe08000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe08400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe08600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe08a00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe08c00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe09000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe09200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe09600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe09800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe09c00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe09e00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0a200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0a400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0a800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0aa00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0ae00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0b000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0b400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0b600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0ba00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0bc00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0c000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0c200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0c600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0c800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0cc00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0ce00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0d200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0d400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0d800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0da00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0de00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0e000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0e400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0e600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0ea00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0ec00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0f000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0f200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0f600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0f800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe0fc00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe0fe00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe10200
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe10400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe10800
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe10a00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe10e00
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe11000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe11400
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Connected all rings
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19715:65287 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19717:65292 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14a0f0000cf0
x3002c0s19b1n0:19717:65292 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-LWtlK5

x3002c0s19b1n0:19717:65292 [3] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19716:65291 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19716:65291 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19716:65291 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19716:65291 [2] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (0)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO ==========================================
x3002c0s19b1n0:19716:65291 [2] NCCL INFO GPU/85000 :GPU/85000 (0/5000.000000/LOC) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19716:65291 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d805600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d805a00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d805c00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d806000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d806200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d806600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d806800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d806c00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d806e00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d807200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d807400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d807800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d807a00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d807e00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d808000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d808400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d808600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d808a00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d808c00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d809000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d809200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d809600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d809800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d809c00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d809e00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80a200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80a400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80a800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80aa00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80ae00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80b000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80b400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80b600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80ba00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80bc00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80c000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80c200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80c600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80c800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80cc00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80ce00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80d200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80d400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80d800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80da00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80de00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80e000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80e400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80e600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80ea00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80ec00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80f000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80f200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80f600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80f800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d80fc00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d80fe00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d810200
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d810400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d810800
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d810a00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d810e00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d811000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d811400
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Connected all rings
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19716:65291 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19714:65289 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19714:65289 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19714:65289 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19714:65289 [0] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO ==========================================
x3002c0s19b1n0:19714:65289 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19714:65289 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba07c00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba08000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba08200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba08600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba08800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba08c00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba08e00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba09200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba09400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba09800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba09a00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba09e00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0a000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0a400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0a600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0aa00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0ac00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0b000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0b200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0b600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0b800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0bc00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0be00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0c200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0c400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0c800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0ca00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0ce00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0d000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0d400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0d600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0da00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0dc00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0e000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0e200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0e600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0e800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0ec00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0ee00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0f200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0f400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0f800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba0fa00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba0fe00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba10000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba10400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba10600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba10a00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba10c00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba11000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba11200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba11600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba11800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba11c00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba11e00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba12200
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba12400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba12800
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba12a00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba12e00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba13000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba13400
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba13600
x3002c0s19b1n0:19714:65289 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba13a00
x3002c0s19b1n0:19717:65285 [3] NCCL INFO Connection to proxy localRank 0 -> connection 0x14a0f0000d10
x3002c0s19b1n0:19717:65285 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14a153811600
x3002c0s19b1n0:19715:65293 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14fdcc000cf0
x3002c0s19b1n0:19715:65293 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-Cu9H8V

x3002c0s19b1n0:19715:65293 [1] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19716:65295 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x1515b0000cf0
x3002c0s19b1n0:19716:65295 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-dOi2L0

x3002c0s19b1n0:19716:65295 [2] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Connected all rings
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19714:65289 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19717:65285 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14a0ee000000
x3002c0s19b1n0:19717:65285 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14a152e00600
x3002c0s19b1n0:19717:65285 [3] NCCL INFO comm 0x561b63a0f020 rank 0 nranks 1 cudaDev 3 busId c7000 - Init COMPLETE
x3002c0s19b1n0:19717:65292 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14a0ea000000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO Connection to proxy localRank 0 -> connection 0x14fdcc000d10
x3002c0s19b1n0:19715:65287 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14fe4fe11600
x3002c0s19b1n0:19715:65293 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14fdc2000000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14fdc6000000
x3002c0s19b1n0:19715:65287 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14fe4f400600
x3002c0s19b1n0:19715:65287 [1] NCCL INFO comm 0x55b1f432f010 rank 0 nranks 1 cudaDev 1 busId 46000 - Init COMPLETE
x3002c0s19b1n0:19714:65296 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14abf0000cf0
x3002c0s19b1n0:19714:65296 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-DIubOU

x3002c0s19b1n0:19714:65296 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19714:65289 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abf0000d10
x3002c0s19b1n0:19714:65289 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14ac7ba13c00
x3002c0s19b1n0:19716:65291 [2] NCCL INFO Connection to proxy localRank 0 -> connection 0x1515b0000d10
x3002c0s19b1n0:19716:65291 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x15160d811600
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f432f010 [nranks=1] stream 0x55b1f1187740

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000034000 seconds
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b63a0f020 [nranks=1] stream 0x561b62231520

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000033000 seconds
x3002c0s19b1n0:19714:65289 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14abee000000
x3002c0s19b1n0:19714:65289 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14ac7b000a00
x3002c0s19b1n0:19714:65289 [0] NCCL INFO comm 0x562df225e2a0 rank 0 nranks 1 cudaDev 0 busId 7000 - Init COMPLETE
x3002c0s19b1n0:19714:65296 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14abe0000000
x3002c0s19b1n0:19716:65295 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x1515a2000000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x1515a6000000
x3002c0s19b1n0:19716:65291 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x15160ce00600
x3002c0s19b1n0:19716:65291 [2] NCCL INFO comm 0x557fa94adf80 rank 0 nranks 1 cudaDev 2 busId 85000 - Init COMPLETE
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df225e2a0 [nranks=1] stream 0x562df0f9e050

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000032000 seconds
x3002c0s19b1n0:19714:19714 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14ac7b000c00
x3002c0s19b1n0:19715:19715 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14fe4f400800
x3002c0s19b1n0:19717:19717 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14a152e00800
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Using network IB
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Using network IB
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Using network IB
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa94adf80 [nranks=1] stream 0x557fa662dd00

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000029000 seconds
x3002c0s19b1n0:19716:19716 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x15160ce00800
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Using network IB
x3002c0s19b1n0:19716:65304 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19716:65304 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19716:65304 [2] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO ==========================================
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19717:65303 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19717:65303 [3] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO ==========================================
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19714:65301 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19714:65301 [0] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO ==========================================
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19715:65302 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO === System : maxBw 12.0 totalBw 264.0 ===
x3002c0s19b1n0:19715:65302 [1] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (1)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + NET[12.5] - NET/1 (605b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + PCI[24.0] - GPU/85000 (2)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/C7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + PCI[24.0] - GPU/C7000 (3)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/85000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/46000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO                             + NVL[88.0] - GPU/7000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO               + NET[12.5] - NET/0 (5c5b02ffffa4e988/1/12.500000)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO ==========================================
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU/46000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (0/5000.000000/LOC) GPU/85000 (1/88.000000/NVL) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (2/24.000000/PHB) CPU/1 (3/24.000000/SYS) CPU/0 (3/24.000000/SYS) NET/1 (4/12.000000/PHB) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU/85000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (0/5000.000000/LOC) GPU/C7000 (1/88.000000/NVL) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) NET/1 (5/12.000000/SYS) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU/C7000 :GPU/7000 (1/88.000000/NVL) GPU/46000 (1/88.000000/NVL) GPU/85000 (1/88.000000/NVL) GPU/C7000 (0/5000.000000/LOC) CPU/3 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) CPU/1 (3/24.000000/SYS) CPU/0 (2/24.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (4/12.000000/PHB) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO NET/1 :GPU/7000 (5/12.000000/SYS) GPU/46000 (4/12.000000/PHB) GPU/85000 (5/12.000000/SYS) GPU/C7000 (5/12.000000/SYS) CPU/3 (3/12.000000/SYS) CPU/2 (2/12.000000/PHB) CPU/1 (3/12.000000/SYS) CPU/0 (3/12.000000/SYS) NET/1 (0/5000.000000/LOC) NET/0 (5/12.000000/SYS) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO NET/0 :GPU/7000 (5/12.000000/SYS) GPU/46000 (5/12.000000/SYS) GPU/85000 (5/12.000000/SYS) GPU/C7000 (4/12.000000/PHB) CPU/3 (3/12.000000/SYS) CPU/2 (3/12.000000/SYS) CPU/1 (3/12.000000/SYS) CPU/0 (2/12.000000/PHB) NET/1 (5/12.000000/SYS) NET/0 (0/5000.000000/LOC) 
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 2, bw 12.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO  0 : NET/1 GPU/1 GPU/0 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO  1 : NET/0 GPU/1 GPU/0 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 2, bw 24.000000/12.000000, type NVL/SYS, sameChannels 1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO  0 : NET/1 GPU/0 GPU/1 GPU/2 GPU/3 NET/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO  1 : NET/0 GPU/0 GPU/1 GPU/2 GPU/3 NET/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 0, bw 0.000000/0.000000, type NVL/PIX, sameChannels 1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Tree 2 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Tree 3 : 0 -> 1 -> 2/-1/-1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Ring 00 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Ring 01 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Ring 02 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Ring 03 : 7 -> 1 -> 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe12c00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe13a00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe13c00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe14a00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe14c00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe15a00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14fe4fe15c00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14fe4fe16a00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Ring 00 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Ring 01 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Ring 02 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Ring 03 : 0 -> 2 -> 3
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
x3002c0s19b1n0:19716:65304 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d812c00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d813a00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d813c00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d814a00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d814c00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d815a00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x15160d815c00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x15160d816a00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14fdb8000c60
x3002c0s19b1n0:19715:65403 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-OywtHW

x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 0 from local rank 1, transport 2
x3002c0s19b1n0:19716:65393 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x151594000c60
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 0 from local rank 2, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000c80
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515b7800000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Ring 00 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Ring 01 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Ring 02 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Ring 03 : 2 -> 3 -> 5
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
x3002c0s19b1n0:19717:65303 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153812c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153813a00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153813c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153814a00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153814c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153815a00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14a153815c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14a153816a00
x3002c0s19b1n0:19717:65394 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14a0dc000c60
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy recv connection 0 from local rank 3, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000c80
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a0ff800000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000c80
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 00/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 1 from local rank 1, transport 2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 1 from local rank 2, transport 0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy recv connection 1 from local rank 3, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000cc0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000cc0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515b9600000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000cc0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a101600000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/4/-1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Tree 2 : 4 -> 0 -> 1/-1/-1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/4/-1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Tree 3 : 4 -> 0 -> 1/-1/-1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 00/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 01/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 02/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 03/04 :    0   2   3   5   4   6   7   1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Ring 00 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Ring 01 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Ring 02 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Ring 03 : 1 -> 0 -> 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x3002c0s19b1n0:19714:65301 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba15200
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba16000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba16200
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba17000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba17200
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba18000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 3456 pointer 0x14ac7ba18200
x3002c0s19b1n0:19714:65301 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 32 pointer 0x14ac7ba19000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 00/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14abd8000c60
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 2 from local rank 2, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000d00
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy recv connection 2 from local rank 3, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000c80
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 01/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 2 from local rank 1, transport 2
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abf5800000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515bb200000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000d00
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a103200000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 01/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000d00
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 1 from local rank 0, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 3 from local rank 2, transport 0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy recv connection 3 from local rank 3, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000cc0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 02/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/1
x3002c0s19b1n0:19715:65302 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 3 from local rank 1, transport 2
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abf7600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000d40
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000d40
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x1515bb800000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000d40
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a103800000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 2 from local rank 0, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-kQRK47

x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 4 from local rank 3, transport 2
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 4 from local rank 2, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 03/0 : 7[c7000] -> 1[46000] [receive] via NET/IB/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000d80
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 5[46000] [send] via NET/IB/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 5 from local rank 3, transport 2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000d80
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515c7200000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000d00
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abfb600000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000dc0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 5[46000] [send] via NET/IB/0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 6 from local rank 3, transport 2
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 5 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 3 from local rank 0, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000dc0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515c7800000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 6 from local rank 2, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000e00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 5[46000] [send] via NET/IB/1
x3002c0s19b1n0:19717:65303 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 7 from local rank 3, transport 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000d40
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abfd600000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000e00
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515c9600000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000e40
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 5[46000] [send] via NET/IB/0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a0dc020000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6988 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a0dc038000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a10f200000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 7 from local rank 2, transport 0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a0dc03a000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6734 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a0dc051000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a111600000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000e40
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fdb801d000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fdb8024000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x1515cb600000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fdb8041000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connected all rings
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fdd3600000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 8 from local rank 2, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 4 from local rank 0, transport 0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fdb8043000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fdb804a000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a0dc053000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fdb8065000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fdd5200000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000e80
x3002c0s19b1n0:19717:65394 [3] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6991 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a0dc06a000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a113600000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x15159b200000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000d80
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14ac2ba00000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fdb8067000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fdb806e000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fdb8089000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fdd7600000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14a0dc06c000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6737 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14a0dc083000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14a117600000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connected all rings
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14fdb808b000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14fdb8092000
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 8 from local rank 3, transport 0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 9 from local rank 2, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 5 from local rank 0, transport 0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14fdb80ad000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14fdd9600000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000ec0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x15159b800000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000e80
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a137600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 4 from local rank 1, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000dc0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14aca0a00000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 10 from local rank 2, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 9 from local rank 3, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000d80
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fdd1800000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 6 from local rank 0, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000e00
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14aca1000000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000ec0
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a179200000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000f00
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151592000000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 5 from local rank 1, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 10 from local rank 3, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 7 from local rank 0, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000f00
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a0da000000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000dc0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fddd600000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000e40
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14abd6000000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 11 from local rank 2, transport 0
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 11 from local rank 3, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 6 from local rank 1, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000f40
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151592600000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000f40
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a0da600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000e00
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fddf600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 7 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000e40
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fde1200000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connected all rings
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 8 from local rank 1, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connected all rings
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 8 from local rank 0, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000e80
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe62c00000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000e80
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abd4000000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 9 from local rank 1, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 9 from local rank 0, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000ec0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abd4600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000ec0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe67400000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 10 from local rank 0, transport 0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 10 from local rank 1, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000f00
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abd4c00000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000f00
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe67a00000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 11 from local rank 0, transport 0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 11 from local rank 1, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000f40
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14abd5200000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000f40
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fe73800000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 12 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000f80
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fdb2000000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 13 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8000fc0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fdb2600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 14 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fdb2c00000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 15 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001040
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14fdb3200000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 16 from local rank 1, transport 0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 12 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-7bX5MS

x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 12 from local rank 0, transport 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000f80
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000f80
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151590600000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001080
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fda8c00000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 13 from local rank 2, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594000fc0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151590c00000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 17 from local rank 1, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 13 from local rank 0, transport 2
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb80010c0
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fda9200000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8000fc0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 14 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 01/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 14 from local rank 0, transport 2
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 18 from local rank 1, transport 0
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001100
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151591200000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fda9800000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy recv connection 15 from local rank 2, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy recv connection 15 from local rank 0, transport 2
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy recv connection 19 from local rank 1, transport 0
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001040
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151591800000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 16 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001040
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001140
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fda6000000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001080
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x15158e000000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 03/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 16 from local rank 0, transport 2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 17 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001080
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 17 from local rank 0, transport 2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x1515940010c0
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x15158e600000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd80010c0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 01/0 : 0[7000] -> 4[7000] [send] via NET/IB/0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 18 from local rank 0, transport 2
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 18 from local rank 2, transport 0
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001100
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/1
x3002c0s19b1n0:19714:65301 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 19 from local rank 0, transport 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001140
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Channel 03/0 : 0[7000] -> 4[7000] [send] via NET/IB/0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14abd803a000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001100
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x15158ec00000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6994 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14abd8052000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14abd3200000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14abd8054000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6740 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14abd806b000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14abca000000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 19 from local rank 2, transport 0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14abd806d000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001140
x3002c0s19b1n0:19714:65401 [0] NCCL INFO NET/IB: Dev 1 Port 1 qpn 6995 mtu 5 GID 0 (80FE/890000FEFF000000)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14abd8084000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14abcaa00000
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x15158f200000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:596 Ib Alloc Size 26560 pointer 0x14abd8086000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 6741 mtu 5 GID 0 (80FE/DE0000FEFF000000)
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:653 Ib Alloc Size 552 pointer 0x14abd809d000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:571 Cuda Host Alloc Size 9641984 pointer 0x14abcb400000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19716:65304 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19716:65304 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19716:65393 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-1n0jx2

x3002c0s19b1n0:19716:65393 [2] NCCL INFO New proxy send connection 20 from local rank 2, transport 2
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19715:65302 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19715:65302 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19715:65403 [1] NCCL INFO New proxy send connection 20 from local rank 1, transport 2
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19717:65303 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19717:65303 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19717:65394 [3] NCCL INFO New proxy send connection 12 from local rank 3, transport 2
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14abd809f000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14abd80a6000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151594001180
x3002c0s19b1n0:19716:65393 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x15158ac00000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fdb8001180
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14abd80c2000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14abc8000000
x3002c0s19b1n0:19715:65403 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14fda4000000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a0dc000f80
x3002c0s19b1n0:19717:65394 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14a0d8600000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14abd80c4000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14abd80cb000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14abd80e6000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14abc8a00000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO Mem Realloc old size 768, new size 1536 pointer 0x14abd80db2c0
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14abd80e8000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14abd80ef000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14abd810a000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14abc9400000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:683 Ib Alloc Size 21688 pointer 0x14abd810c000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:696 Ib Alloc Size 552 pointer 0x14abd8113000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net_ib.cc:771 Ib Alloc Size 552 pointer 0x14abd812e000
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:698 Cuda Host Alloc Size 9641984 pointer 0x14abc6000000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Latency/AlgBw |    Tree/    LL |    Tree/ LL128 |    Tree/Simple |    Ring/    LL |    Ring/ LL128 |    Ring/Simple | CollNetDirect/    LL | CollNetDirect/ LL128 | CollNetDirect/Simple | CollNetChain/    LL | CollNetChain/ LL128 | CollNetChain/Simple |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO  Max NThreads |            512 |            640 |            512 |            512 |            640 |            512 |              0 |              0 |            512 |              0 |              0 |            512 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO     Broadcast |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO        Reduce |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     6.3/   6.0 |    14.0/   0.0 |    18.0/  24.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO     AllGather |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO ReduceScatter |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |    11.4/   6.9 |    25.4/   0.0 |    38.4/  27.4 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |     0.0/   0.0 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO     AllReduce |    20.0/   5.8 |    28.9/   0.0 |   224.0/  22.1 |    22.9/   3.4 |    45.0/   0.0 |    80.8/  13.7 |     7.4/   0.0 |     7.4/   0.0 |    29.7/   0.0 |     4.4/   0.0 |     4.4/   0.0 |    48.0/   0.0 |
x3002c0s19b1n0:19714:65301 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x3002c0s19b1n0:19714:65301 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
x3002c0s19b1n0:19714:65401 [0] NCCL INFO New proxy send connection 20 from local rank 0, transport 2
x3002c0s19b1n0:19714:65301 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abd8001180
x3002c0s19b1n0:19714:65301 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14ac7ba19200
x3002c0s19b1n0:19715:65302 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14fe4fe16c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14a153816c00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14a0d6000000
x3002c0s19b1n0:19717:65303 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14a152e00a00
x3002c0s19b1n0:19717:65303 [3] NCCL INFO comm 0x561b6504ffd0 rank 3 nranks 8 cudaDev 3 busId c7000 - Init COMPLETE
x3002c0s19b1n0:19715:65302 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14fda2000000
x3002c0s19b1n0:19715:65302 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14fe4f400a00
x3002c0s19b1n0:19715:65302 [1] NCCL INFO comm 0x55b1f324f740 rank 1 nranks 8 cudaDev 1 busId 46000 - Init COMPLETE
x3002c0s19b1n0:19716:65304 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x15160d816c00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x151582000000
x3002c0s19b1n0:19716:65304 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x15160ce00a00
x3002c0s19b1n0:19716:65304 [2] NCCL INFO comm 0x557fa98352d0 rank 2 nranks 8 cudaDev 2 busId 85000 - Init COMPLETE
x3002c0s19b1n0:19714:65301 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14abc4000000
x3002c0s19b1n0:19714:65301 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14ac7b000e00
x3002c0s19b1n0:19714:65301 [0] NCCL INFO comm 0x562df2a4f2f0 rank 0 nranks 8 cudaDev 0 busId 7000 - Init COMPLETE
x3002c0s19b1n0:19714:65401 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 8388608 pointer 0x14abc6a00000
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df2a4f2f0 [nranks=8] stream 0x562df0f9e190
x3002c0s19b1n0:19714:19714 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f324f740 [nranks=8] stream 0x55b1f1187880
x3002c0s19b1n0:19715:19715 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b6504ffd0 [nranks=8] stream 0x561b6220f0c0
x3002c0s19b1n0:19717:19717 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa98352d0 [nranks=8] stream 0x557fa662de40
x3002c0s19b1n0:19716:19716 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 25799 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa94adf80 [nranks=1] stream 0x557fa662dd00

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000033000 seconds
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 1 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa98352d0 [nranks=8] stream 0x557fa662de40
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 76343 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f432f010 [nranks=1] stream 0x55b1f1187740

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000041000 seconds
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 1 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f324f740 [nranks=8] stream 0x55b1f1187880
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 66595 us
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b63a0f020 [nranks=1] stream 0x561b62231520

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000037000 seconds
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 1 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b6504ffd0 [nranks=8] stream 0x561b6220f0c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 89308 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_3200ns_512sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_3200ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_train_indexmap_3200ns_512sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 40959
    total number of epochs: 1
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df225e2a0 [nranks=1] stream 0x562df0f9e050

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000033000 seconds
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 1 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df2a4f2f0 [nranks=8] stream 0x562df0f9e190

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000024000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000000000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 73375 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 72579 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 57218 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_352ns_512sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_352ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_valid_indexmap_352ns_512sl_1234s_shuffle_idx.npy
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 1 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f432f010 [nranks=1] stream 0x55b1f1187740

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000034000 seconds
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 2 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 4 op 0 root 0 comm 0x55b1f324f740 [nranks=8] stream 0x55b1f1187880
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 65463 us
    loaded indexed file in 0.012 seconds
    total number of samples: 1406
    total number of epochs: 1
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df225e2a0 [nranks=1] stream 0x562df0f9e050

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000040000 seconds
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 2 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 4 op 0 root 0 comm 0x562df2a4f2f0 [nranks=8] stream 0x562df0f9e190

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000023000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000013000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 8
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 2 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa94adf80 [nranks=1] stream 0x557fa662dd00

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000033000 seconds
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 2 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 4 op 0 root 0 comm 0x557fa98352d0 [nranks=8] stream 0x557fa662de40
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 3 ncclAllReduce: count: 1, datatype: ncclInt64, message size: 8 Bytes
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b63a0f020 [nranks=1] stream 0x561b62231520

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000033000 seconds
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 2 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 4 op 0 root 0 comm 0x561b6504ffd0 [nranks=8] stream 0x561b6220f0c0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum int64_t Elapsed time: 11676 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum int64_t Elapsed time: 45993 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum int64_t Elapsed time: 26078 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum int64_t Elapsed time: 29218 us
 > loading doc-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_32ns_512sl_1234s_doc_idx.npy
x3002c0s19b1n0:19715:19715 [1] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14fe4f400c00
x3002c0s19b1n0:19716:19716 [2] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x15160ce00c00
 > loading sample-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_32ns_512sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/yuke/lyd/Megatron-LM/my-gpt2_text_document_test_indexmap_32ns_512sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.030 seconds
    total number of samples: 48
    total number of epochs: 2
> finished creating GPT datasets ...
x3002c0s19b1n0:19717:19717 [3] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14a152e00c00
x3002c0s19b1n0:19714:19714 [0] NCCL INFO init.cc:1147 Cuda Host Alloc Size 4 pointer 0x14ac7b001000
x3002c0s19b1n0:19716:680 [2] NCCL INFO Using network IB
x3002c0s19b1n0:19717:682 [3] NCCL INFO Using network IB
x3002c0s19b1n0:19715:678 [1] NCCL INFO Using network IB
x3002c0s19b1n0:19714:686 [0] NCCL INFO Using network IB
x3002c0s19b1n0:19716:680 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19716:680 [2] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19716:680 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19716:680 [2] NCCL INFO GPU Direct RDMA Disabled for GPU 85000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19716:680 [2] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19716:680 [2] NCCL INFO CPU/1 (1/2/-1)
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:680 [2] NCCL INFO + PCI[24.0] - PCI/83000 (11f8406811f8beef)
x3002c0s19b1n0:19716:680 [2] NCCL INFO               + PCI[24.0] - GPU/85000 (0)
x3002c0s19b1n0:19716:680 [2] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19716:680 [2] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19716:680 [2] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/1
x3002c0s19b1n0:19716:680 [2] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19716:680 [2] NCCL INFO ==========================================
x3002c0s19b1n0:19716:680 [2] NCCL INFO GPU/85000 :GPU/85000 (0/5000.000000/LOC) CPU/1 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19716:680 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:680 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:680 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19716:680 [2] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19716:680 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19716:680 [2] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d818200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d818600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d818800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d818c00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d818e00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d819200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d819400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d819800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d819a00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d819e00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81a000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81a400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81a600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81aa00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81ac00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81b000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81b200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81b600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81b800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81bc00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81be00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81c200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81c400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81c800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81ca00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81ce00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81d000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81d400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81d600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81da00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81dc00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81e000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81e200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81e600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81e800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81ec00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81ee00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81f200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81f400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81f800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d81fa00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d81fe00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d820000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d820400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d820600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d820a00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d820c00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d821000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d821200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d821600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d821800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d821c00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d821e00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d822200
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d822400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d822800
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d822a00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d822e00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d823000
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d823400
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d823600
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d823a00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x15160d823c00
x3002c0s19b1n0:19716:680 [2] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x15160d824000
x3002c0s19b1n0:19716:680 [2] NCCL INFO Connected all rings
x3002c0s19b1n0:19716:680 [2] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19716:680 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19717:682 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19717:682 [3] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19717:682 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 0 (distance 6 > 4)
x3002c0s19b1n0:19717:682 [3] NCCL INFO GPU Direct RDMA Disabled for GPU c7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19717:682 [3] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19717:682 [3] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19717:682 [3] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19717:682 [3] NCCL INFO + PCI[24.0] - PCI/C5000 (11f8406811f8beef)
x3002c0s19b1n0:19717:682 [3] NCCL INFO               + PCI[24.0] - GPU/C7000 (0)
x3002c0s19b1n0:19717:682 [3] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19717:682 [3] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19717:682 [3] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19717:682 [3] NCCL INFO ==========================================
x3002c0s19b1n0:19717:682 [3] NCCL INFO GPU/C7000 :GPU/C7000 (0/5000.000000/LOC) CPU/0 (2/24.000000/PHB) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19717:682 [3] NCCL INFO Setting affinity for GPU 3 to 01
x3002c0s19b1n0:19717:682 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:682 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:682 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19717:682 [3] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19717:682 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19717:682 [3] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153818200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153818600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153818800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153818c00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153818e00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153819200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153819400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153819800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153819a00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153819e00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381a000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381a400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381a600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381aa00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381ac00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381b000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381b200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381b600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381b800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381bc00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381be00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381c200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381c400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381c800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381ca00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381ce00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381d000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381d400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381d600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381da00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381dc00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381e000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381e200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381e600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381e800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381ec00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381ee00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381f200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381f400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381f800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a15381fa00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a15381fe00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153820000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153820400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153820600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153820a00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153820c00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153821000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153821200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153821600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153821800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153821c00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153821e00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153822200
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153822400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153822800
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153822a00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153822e00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153823000
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153823400
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153823600
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153823a00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14a153823c00
x3002c0s19b1n0:19717:682 [3] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14a153824000
x3002c0s19b1n0:19717:682 [3] NCCL INFO Connected all rings
x3002c0s19b1n0:19717:682 [3] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19717:682 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19715:678 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19715:678 [1] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19715:678 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 1 (distance 6 > 4)
x3002c0s19b1n0:19715:678 [1] NCCL INFO GPU Direct RDMA Disabled for GPU 46000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19715:678 [1] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19715:678 [1] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19715:678 [1] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO + PCI[24.0] - PCI/44000 (11f8406811f8beef)
x3002c0s19b1n0:19715:678 [1] NCCL INFO               + PCI[24.0] - GPU/46000 (0)
x3002c0s19b1n0:19715:678 [1] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19715:678 [1] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19715:678 [1] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19715:678 [1] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19715:678 [1] NCCL INFO ==========================================
x3002c0s19b1n0:19715:678 [1] NCCL INFO GPU/46000 :GPU/46000 (0/5000.000000/LOC) CPU/2 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) 
x3002c0s19b1n0:19715:678 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:678 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:678 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19715:678 [1] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19715:678 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19715:678 [1] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe18200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe18600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe18800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe18c00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe18e00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe19200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe19400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe19800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe19a00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe19e00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1a000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1a400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1a600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1aa00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1ac00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1b000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1b200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1b600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1b800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1bc00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1be00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1c200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1c400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1c800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1ca00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1ce00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1d000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1d400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1d600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1da00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1dc00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1e000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1e200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1e600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1e800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1ec00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1ee00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1f200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1f400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1f800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe1fa00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe1fe00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe20000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe20400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe20600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe20a00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe20c00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe21000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe21200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe21600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe21800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe21c00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe21e00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe22200
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe22400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe22800
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe22a00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe22e00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe23000
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe23400
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe23600
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe23a00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14fe4fe23c00
x3002c0s19b1n0:19715:678 [1] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14fe4fe24000
x3002c0s19b1n0:19715:678 [1] NCCL INFO Connected all rings
x3002c0s19b1n0:19715:678 [1] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19715:678 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19714:686 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 0 'mlx5_0'
x3002c0s19b1n0:19714:686 [0] NCCL INFO NET/IB : GPU Direct RDMA Enabled for HCA 1 'mlx5_1'
x3002c0s19b1n0:19714:686 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 0 (distance 7 > 4)
x3002c0s19b1n0:19714:686 [0] NCCL INFO GPU Direct RDMA Disabled for GPU 7000 / HCA 1 (distance 7 > 4)
x3002c0s19b1n0:19714:686 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
x3002c0s19b1n0:19714:686 [0] NCCL INFO CPU/3 (1/2/-1)
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:686 [0] NCCL INFO + PCI[24.0] - PCI/5000 (11f8406811f8beef)
x3002c0s19b1n0:19714:686 [0] NCCL INFO               + PCI[24.0] - GPU/7000 (0)
x3002c0s19b1n0:19714:686 [0] NCCL INFO CPU/0 (1/2/-1)
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/2
x3002c0s19b1n0:19714:686 [0] NCCL INFO + PCI[12.0] - NIC/C8000
x3002c0s19b1n0:19714:686 [0] NCCL INFO CPU/2 (1/2/-1)
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/3
x3002c0s19b1n0:19714:686 [0] NCCL INFO + SYS[5000.0] - CPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO + PCI[12.0] - NIC/43000
x3002c0s19b1n0:19714:686 [0] NCCL INFO ==========================================
x3002c0s19b1n0:19714:686 [0] NCCL INFO GPU/7000 :GPU/7000 (0/5000.000000/LOC) CPU/3 (2/24.000000/PHB) CPU/0 (3/24.000000/SYS) CPU/2 (3/24.000000/SYS) 
x3002c0s19b1n0:19714:686 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:686 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:686 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 44.000000/44.000000, type LOC/PIX, sameChannels 1
x3002c0s19b1n0:19714:686 [0] NCCL INFO  0 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  1 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  2 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  3 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  4 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  5 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  6 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  7 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  8 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO  9 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 10 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 11 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 12 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 13 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 14 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO 15 : GPU/0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 00/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 01/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 02/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 03/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 04/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 05/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 06/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 07/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 08/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 09/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 10/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 11/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 12/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 13/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 14/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 15/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 16/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 17/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 18/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 19/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 20/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 21/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 22/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 23/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 24/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 25/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 26/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 27/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 28/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 29/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 30/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Channel 31/32 :    0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
x3002c0s19b1n0:19714:686 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
x3002c0s19b1n0:19714:686 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1a800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1ac00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1ae00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1b200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1b400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1b800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1ba00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1be00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1c000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1c400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1c600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1ca00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1cc00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1d000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1d200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1d600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1d800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1dc00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1de00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1e200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1e400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1e800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1ea00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1ee00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1f000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1f400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1f600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba1fa00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba1fc00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba20000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba20200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba20600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba20800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba20c00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba20e00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba21200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba21400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba21800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba21a00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba21e00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba22000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba22400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba22600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba22a00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba22c00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba23000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba23200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba23600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba23800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba23c00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba23e00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba24200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba24400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba24800
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba24a00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba24e00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba25000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba25400
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba25600
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba25a00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba25c00
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba26000
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:23 Cuda Alloc Size 768 pointer 0x14ac7ba26200
x3002c0s19b1n0:19714:686 [0] NCCL INFO channel.cc:27 Cuda Alloc Size 4 pointer 0x14ac7ba26600
x3002c0s19b1n0:19714:686 [0] NCCL INFO Connected all rings
x3002c0s19b1n0:19714:686 [0] NCCL INFO Connected all trees by HAO
x3002c0s19b1n0:19714:686 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
x3002c0s19b1n0:19717:696 [3] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14a0e4000bb0
x3002c0s19b1n0:19717:696 [3] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-U1JcK4

x3002c0s19b1n0:19717:696 [3] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19716:695 [2] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x15157c000cf0
x3002c0s19b1n0:19716:695 [2] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-Z2FxH1

x3002c0s19b1n0:19716:695 [2] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19715:698 [1] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14fd9c000cf0
x3002c0s19b1n0:19715:698 [1] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-cL9aRV

x3002c0s19b1n0:19715:698 [1] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19714:699 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x14abbc000cf0
x3002c0s19b1n0:19714:699 [0] NCCL INFO Allocated 4194656 bytes of shared memory in /dev/shm/nccl-RioSUQ

x3002c0s19b1n0:19714:699 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
x3002c0s19b1n0:19717:682 [3] NCCL INFO Connection to proxy localRank 0 -> connection 0x14a0e426af10
x3002c0s19b1n0:19717:682 [3] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14a153824200
x3002c0s19b1n0:19716:680 [2] NCCL INFO Connection to proxy localRank 0 -> connection 0x15157c000d10
x3002c0s19b1n0:19716:680 [2] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x15160d824200
x3002c0s19b1n0:19714:686 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14abbc000d10
x3002c0s19b1n0:19714:686 [0] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14ac7ba26800
x3002c0s19b1n0:19715:678 [1] NCCL INFO Connection to proxy localRank 0 -> connection 0x14fd9c000d10
x3002c0s19b1n0:19715:678 [1] NCCL INFO init.cc:367 Cuda Alloc Size 5168 pointer 0x14fe4fe24200
x3002c0s19b1n0:19717:696 [3] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14a0c0000000
x3002c0s19b1n0:19717:682 [3] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14a0ce000000
x3002c0s19b1n0:19717:682 [3] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14a152e00e00
x3002c0s19b1n0:19717:682 [3] NCCL INFO comm 0x561b6432ada0 rank 0 nranks 1 cudaDev 3 busId c7000 - Init COMPLETE
x3002c0s19b1n0:19716:680 [2] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x151572000000
x3002c0s19b1n0:19716:680 [2] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x15160ce00e00
x3002c0s19b1n0:19716:680 [2] NCCL INFO comm 0x557fa87c6cd0 rank 0 nranks 1 cudaDev 2 busId 85000 - Init COMPLETE
x3002c0s19b1n0:19716:695 [2] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x15156e000000
x3002c0s19b1n0:19714:699 [0] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14abb0000000
x3002c0s19b1n0:19714:686 [0] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14abba000000
x3002c0s19b1n0:19714:686 [0] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14ac7b001200
x3002c0s19b1n0:19714:686 [0] NCCL INFO comm 0x562df3ac2f40 rank 0 nranks 1 cudaDev 0 busId 7000 - Init COMPLETE
x3002c0s19b1n0:19715:698 [1] NCCL INFO transport/net.cc:376 Cuda Alloc Size 67108864 pointer 0x14fd8e000000
x3002c0s19b1n0:19715:678 [1] NCCL INFO init.cc:392 Cuda Host Alloc Size 33554432 pointer 0x14fd92000000
x3002c0s19b1n0:19715:678 [1] NCCL INFO init.cc:398 Cuda Host Alloc Size 128 pointer 0x14fe4f400e00
x3002c0s19b1n0:19715:678 [1] NCCL INFO comm 0x55b1f2a1c430 rank 0 nranks 1 cudaDev 1 busId 46000 - Init COMPLETE
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 1 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3002c0s19b1n0:19715:19715 [1] NCCL INFO Broadcast: opCount 0 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 24 datatype 0 op 0 root 0 comm 0x55b1f2a1c430 [nranks=1] stream 0x55b1f11879c0
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 3 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3002c0s19b1n0:19717:19717 [3] NCCL INFO Broadcast: opCount 0 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 24 datatype 0 op 0 root 0 comm 0x561b6432ada0 [nranks=1] stream 0x561b6220f200
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 24 datatype 0 op 0 root 0 comm 0x562df3ac2f40 [nranks=1] stream 0x562df0e364c0
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 2 Broadcast: count: 3, datatype: ncclInt64, message size: 24 Bytes
x3002c0s19b1n0:19716:19716 [2] NCCL INFO Broadcast: opCount 0 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 24 datatype 0 op 0 root 0 comm 0x557fa87c6cd0 [nranks=1] stream 0x557fa660c550
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 7 sendbuff 0x14fe4ba28800 recvbuff 0x14fe4ba28800 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 7 sendbuff 0x14a193628800 recvbuff 0x14a193628800 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 7 sendbuff 0x15164f628800 recvbuff 0x15164f628800 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 7 sendbuff 0x14acbb628800 recvbuff 0x14acbb628800 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000055000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000017000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb628800
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb628800
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 60454 us
[after dataloaders are built] datetime: 2023-11-08 02:02:02 
done with setup ...
training ...
[Hao INFO in ncclAllReduce (all_reduce.cc)] rank 0 ncclAllReduce: count: 1, datatype: ncclUint8, message size: 1 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO AllReduce: opCount 8 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 1 datatype 1 op 0 root 0 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0

[LYD INFO in ncclAllReduce (all_reduce.cc)] rank 0 AllReduce Time: 0.000049000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 0

[LYD INFO in getAlgoInfo (enqueue.cc)] algorithm a is: 1

[LYD INFO in getAlgoInfo (enqueue.cc)] rank: 0, Duration between Point 1 and Point 2: 0.000014000 seconds

[LYD INFO in getAlgoInfo (enqueue.cc)] rank 0, Duration between Point 2 and Point 3: 0.000001000 seconds
[LYD INFO in getAlgoInfo (enqueue.cc)] out rank: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out coll: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sendbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out recvbuff address: 0x14acbb600400
[LYD INFO in getAlgoInfo (enqueue.cc)] out count: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out datatype: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSteps: 4
[LYD INFO in getAlgoInfo (enqueue.cc)] out sliceSteps: 2
[LYD INFO in getAlgoInfo (enqueue.cc)] out algorithm: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out protocol: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nChannels: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nThreads: 96
[LYD INFO in getAlgoInfo (enqueue.cc)] out nBytes: 1
[LYD INFO in getAlgoInfo (enqueue.cc)] out nstepsPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out nchunksPerLoop: 0
[LYD INFO in getAlgoInfo (enqueue.cc)] out chunkSize: 0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 85619 us
x3002c0s19b1n0:19716:19716 [2] NCCL INFO AllReduce: opCount 8 sendbuff 0x15164f600400 recvbuff 0x15164f600400 count 1 datatype 1 op 0 root 0 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 105992 us
x3002c0s19b1n0:19717:19717 [3] NCCL INFO AllReduce: opCount 8 sendbuff 0x14a193600400 recvbuff 0x14a193600400 count 1 datatype 1 op 0 root 0 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 139744 us
x3002c0s19b1n0:19715:19715 [1] NCCL INFO AllReduce: opCount 8 sendbuff 0x14fe4ba00400 recvbuff 0x14fe4ba00400 count 1 datatype 1 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4
[LYD INFO in NCCL_KERN_NAME (common.h)] 2 AllReduce TREE LL Sum uint8_t Elapsed time: 34225 us
[Hao INFO in ncclRecv (sendrecv.cc)] rank 2 Recv: count: 19660800, datatype: ncclFloat16/ncclHalf, message size: 39321600 Bytes
x3002c0s19b1n0:19716:19716 [2] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x151544000000 count 19660800 datatype 6 op 0 root 1 comm 0x557f892b8060 [nranks=8] stream 0x557fa662dbc0

[LYD INFO in NCCL_KERN_NAME (common.h)] 0 AllReduce TREE LL Sum uint8_t Elapsed time: 53210 us
[before the start of training step] datetime: 2023-11-08 02:02:02 
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 5, datatype: ncclInt64, message size: 40 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14acbb600400 recvbuff 0x14acbb600400 count 40 datatype 0 op 0 root 0 comm 0x562df3ac2f40 [nranks=1] stream 0x562df0e364c0
[Hao INFO in ncclBroadcast (broadcast.cc)] rank 0 Broadcast: count: 16416, datatype: ncclInt64, message size: 131328 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x14acbb681e00 recvbuff 0x14acbb681e00 count 131328 datatype 0 op 0 root 0 comm 0x562df3ac2f40 [nranks=1] stream 0x562df0e364c0

[LYD INFO in NCCL_KERN_NAME (common.h)] 1 AllReduce TREE LL Sum uint8_t Elapsed time: 13726 us

[LYD INFO in NCCL_KERN_NAME (common.h)] 3 AllReduce TREE LL Sum uint8_t Elapsed time: 24837 us
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 21 from local rank 2, transport 0
[Hao INFO in ncclRecv (sendrecv.cc)] rank 3 Recv: count: 19660800, datatype: ncclFloat16/ncclHalf, message size: 39321600 Bytes
x3002c0s19b1n0:19717:19717 [3] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x14a09c000000 count 19660800 datatype 6 op 0 root 2 comm 0x561b4ac019d0 [nranks=8] stream 0x561b6225cfc0
[Hao INFO in ncclRecv (sendrecv.cc)] rank 1 Recv: count: 19660800, datatype: ncclFloat16/ncclHalf, message size: 39321600 Bytes
x3002c0s19b1n0:19715:19715 [1] NCCL INFO Recv: opCount 9 sendbuff (nil) recvbuff 0x14fd64000000 count 19660800 datatype 6 op 0 root 0 comm 0x55b1d39c5440 [nranks=8] stream 0x55b1f11a8ff0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 13 from local rank 3, transport 0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 21 from local rank 1, transport 0
x3002c0s19b1n0:19716:1698 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x1516080011c0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151546600000
x3002c0s19b1n0:19715:1700 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe040011c0
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fd66600000
x3002c0s19b1n0:19717:1699 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144000fc0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a09e600000
x3002c0s19b1n0:19715:63481 [1] NCCL INFO New proxy recv connection 22 from local rank 1, transport 0
x3002c0s19b1n0:19717:63484 [3] NCCL INFO New proxy recv connection 14 from local rank 3, transport 0
x3002c0s19b1n0:19716:63485 [2] NCCL INFO New proxy recv connection 22 from local rank 2, transport 0
x3002c0s19b1n0:19715:1700 [1] NCCL INFO Connection to proxy localRank 1 -> connection 0x14fe04001200
x3002c0s19b1n0:19715:63481 [1] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14fd66c00000
x3002c0s19b1n0:19717:1699 [3] NCCL INFO Connection to proxy localRank 3 -> connection 0x14a144001000
x3002c0s19b1n0:19717:63484 [3] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x14a09ec00000
x3002c0s19b1n0:19716:1698 [2] NCCL INFO Connection to proxy localRank 2 -> connection 0x151608001200
x3002c0s19b1n0:19716:63485 [2] NCCL INFO transport/p2p.cc:449 Cuda Alloc Size 6291456 pointer 0x151546c00000
[Hao INFO in ncclSend (sendrecv.cc)] rank 0 Send: count: 19660800, datatype: ncclFloat16/ncclHalf, message size: 39321600 Bytes
x3002c0s19b1n0:19714:19714 [0] NCCL INFO Send: opCount 9 sendbuff (nil) recvbuff 0x14a98eb00000 count 19660800 datatype 6 op 0 root 1 comm 0x562dd1e1f3a0 [nranks=8] stream 0x562df0fbf7a0
x3002c0s19b1n0:19714:1967 [0] NCCL INFO Channel 00/1 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 21 from local rank 0, transport 0
x3002c0s19b1n0:19714:1967 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c0011c0
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a91b600000
x3002c0s19b1n0:19714:1967 [0] NCCL INFO Channel 01/1 : 0[7000] -> 1[46000] via P2P/IPC/read
x3002c0s19b1n0:19714:63482 [0] NCCL INFO New proxy send connection 22 from local rank 0, transport 0
x3002c0s19b1n0:19714:1967 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x14ac6c001200
x3002c0s19b1n0:19714:63482 [0] NCCL INFO transport/p2p.cc:430 Cuda Alloc Size 6291456 pointer 0x14a925600000
[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4[LYD INFO in ncclLaunchKernel (enqueue.cc)] NCHANNELS is: 4